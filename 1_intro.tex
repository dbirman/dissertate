
\section{Sensory selection}

We all know, more or less intuitively, what it is like to attend to something that we see. Hikers, for example, when on a dangerous trail may be particularly motivated to focus on the stability of their foot placements to avoid stepping on a loose rock, while ignoring the sights and sounds of the forests around them. The ease with which we can deploy attention hides a complex set of changes which occur inside the brain and which change our sensory perceptions.

In early attention research, observers used introspection to evaluate how their sensory experiences changed when they focused in various ways. These early researchers found that attention could change the conscious perception of a stimulus, making it more intense or clear \citep{Helmholtz1924-rl,James1981-cj,Kuelpe1902-qz,Titchener1908-bx}. These intuitions were not without merit, as spatial attention to some stimuli does appear to lead to a change in the appearance of a stimulus \citep{Carrasco2018-sb}. But without concepts to tackle the ideas of mechanisms and computation these authors never succeeded in making headway into how attention might be implemented in the human brain. 

Introspection yielded in the middle of the 20th century to more quantitative approaches in which attention, broadly defined, was operationalized into specific tasks. This allowed researchers to begin to make claims about the algorithm of attention \citep{Marr1982-fg}. In particular, psychophysicists in the mid-20th century used reports about unattended stimuli to understand how information was selected. Perhaps the earliest example of this is a set of experiments in which observers were asked to echo speech from one ear at a time \citep{Cherry1953-as}. The transition from introspection to ‘objective’ measurement can be highlighted in the way these experiments were introduced, quoting from Cherry (1953): ``the `subject' under test (the listener) is regarded as a transducer whose responses are observed when various stimuli are applied, whereas his subjective impressions are taken to be of minor importance''. This approach was effective. The field made an abrupt move toward understanding sensory systems as processing units. Any given stimulus could be processed in a serial or parallel manner and processing might go to `completion' or be halted at a certain point. Measuring the knowledge of a subject about an attended or ignored stimulus therefore assesses the extent of processing which has occurred for that stimulus. For example, for the echoing task described above, an observer might be asked whether they had retained knowledge about an un-echoed (and therefore, unattended) voice. If they could recall simple low-level features, e.g. that the speaker had a higher-pitched voice, then a researcher could conclude that parallel processing of both streams had occurred up to pitch processing [cite?]. They could then also conclude that processing of the ignored stimulus had then been halted, before the point of complete semantic understanding. Such results led researchers to begin to distinguish between stages of processing: an early parallel stage in which incoming sensory information is processed without immediate limits in capacity, and a second limited capacity serial stage from which complex decisions could be made.

Several early theories of attention focused on when selection occurs relative to the parallel and serial stages of processing. These theories fell out of the observations made above, that some features of auditory, as well as other sensory, stimuli are available for decision making regardless of the observers focus while other features must be attended to be available. To reconcile these facts researchers suggested a bottleneck and proposed that this was the mechanistic implementation of selective attention. The first such modern theory was Broadbent’s Filter theory \citep{Broadbent1958-ny}, which included an early bottleneck. In Filter theory visual information is processed in parallel until low-level features (location, intensity, frequency) are resolved. At this point, parallel processing gives way to a serial ‘complete’ processing of object identity, form, etc. The alternative theory, late selection, suggests that processing occurs up to semantics in an unconscious parallel manner \citep{Deutsch1963-ac}. An example best illustrates the distinctions in these theories. Again, if an observer is echoing one speaker while ignoring a second, a late selection account predicts that a substantial amount of information is nevertheless available about the ignored voice. This is because late selection predicts that a semantic-level processing of speech will occur regardless of sensory selection, leaving high-level information available to an observer. Evidence for this comes from experiments in which observers orient to highly salient, but also very high-level features, such as their own name, even when focused on other tasks \citep{Moray1959-fn}. This suggests that not also features are processed in identical ways. Selection theories were expanded upon, based on this suggestion that features might differ in their form of processing and not be consistently selected out in one step. Instead, it was suggested that features might be attenuated \citep{Treisman1960-qs} or that selection might be limited by variable capacity at different stages of processing \citep{Kahneman1973-af}.

Selective attention has been most heavily studied in the visual domain, in part because of the close relationship between selecting information in the world and orienting of the eyes. Spatial orienting can occur both in an overt movement of the eyes, but also covertly without eye movements. This covert attention accelerates responses \citep{Eriksen1972-qj,Posner1980-cr} and both improves detection performance and increases discrimination sensitivity \citep{Carrasco2011-xp}. Cues about important locations can both be imposed externally on an observer \citep{Posner1980-wb} or the result of internal guiding of attention toward the cued location. Covert attention be either spatial, related to eye movements, or featural \citep{Rossi1995-wa} as described below for search tasks. Because of this close relationship between selection with eye movements and vision, researchers have focused in particular on the implementation of selective attention in the visual system. 

\section{Implementations of selective visual attention}

Selective attention is a balancing act for the brain, which must weigh the possibility of needing unattended information against the strength of sensory selection. In an early selection theory, information is being thrown out before complete processing -- which could be potentially disadvantageous if that stimulus later becomes important for behavior \citep{Mack1998-nq}. Late 20th century research on attention was well aware of this fact, and researchers showed that in different contexts sensory selection can be more or less complete at filtering out irrelevant information. In a few particularly dramatic demonstrations \citep{Haines1991-si,Mack1998-nq,Neisser1979-mm,Simons1999-ng} observers can be shown to entirely lose access to otherwise highly salient information, but only when performing a task with significant cognitive load \citep{Lavie2005-aw,Lavie2004-ub,Rees1997-hd}. In vision, like in audition \citep{Moray1959-fn}, some visual information is processed no matter what, as scene gist can survive inattention, perceptually \citep{Li2002-ji} and as decodable information from measurements of BOLD signal in visual cortex \citep{Peelen2009-us}. These examples highlight the delicate balance that the brain must maintain: with limited resources how should the brain allocate these to efficiently sample the sensory world? Early research on the implementation of attention put this question to the side while first exploring the possible effects of attention on sensory processing. In apparent contradiction with the specific bottlenecks proposed by early and late selection theories, physiologists found that behavioral selection results in neural changes throughout the brain: in early visual cortex (where, presumably `parallel' processing occurs), higher visual cortex, and in non-sensory regions. Reconciling these observations with the behavioral effects of selective attention has become a major goal of cognitive neuroscience. 

Modern neuroscience now deploys a multitude of neural recording techniques to understand how behavioral selection might be implemented by the brain. Both at a coarse scale in humans and at the level of individual neurons in primates and, very recently, rodents. In humans and non-human primates attention has been shown to alter the response gain of neurons in the visual system, including in the LGN \citep{OConnor2002-mx}, in V1 \citep{Motter1993-av}, V2 \citep{Buffalo2010-lr,Luck1997-sq,Motter1993-av}, V3 (Liu, Larsson, & Carrasco, 2007; Pestilli, Carrasco, Heeger, & Gardner, 2011; Saenz, Buracas, & Boynton, 2002; Silver, Ress, & Heeger, 2007), V4 (Buffalo et al., 2010; Connor, Gallant, Preddie, & Van Essen, 1996; Luck et al., 1997; McAdams & Maunsell, 1999; Moran & Desimone, 1985; Motter, 1993; Reynolds, Pasternak, & Desimone, 2000; Spitzer, Desimone, & Moran, 1988), V3A (Serences & Boynton, 2007),  MT (Beauchamp, Cox, & DeYoe, 1997; O’Craven, Rosen, Kwong, Treisman, & Savoy, 1997; Saenz et al., 2002; Seidemann, Poirson, Wandell, & Newsome, 1999; Serences & Boynton, 2007; Treue & Martínez Trujillo, 1999; Treue & Maunsell, 1996) and MST (O’Craven et al., 1997; Treue & Maunsell, 1996), and in IT cortex (Chelazzi, Duncan, Miller, & Desimone, 1998; Moran & Desimone, 1985). Using BOLD imaging these changes can be observed simultaneously throughout almost all of early visual cortex (Liu et al., 2007; Pestilli et al., 2011; Saenz et al., 2002; Silver et al., 2007) and ventral temporal cortex \citep{Baldauf2014-uj}. Although the neurochemical mechanisms of these effects remain unclear, muscarinic acetylcholine receptors are known to play a key role \citep{Herrero2008-am}. Changes to sensory representations occur both for spatial attention tasks (Klein, Harvey, & Dumoulin, 2014; McAdams & Maunsell, 1999; Mitchell, Sundberg, & Reynolds, 2009; Pestilli et al., 2011; Womelsdorf, Anton-Erxleben, Pieper, & Treue, 2006) and feature-based attention tasks (Baldauf & Desimone, 2014; Harel, Kravitz, & Baker, 2014; Huk & Heeger, 2000; Jehee, Brady, & Tong, 2011; Saenz et al., 2002; Sàenz, Buraĉas, & Boynton, 2003; Serences & Boynton, 2007; Treue & Martínez Trujillo, 1999).

As measurements of physiology have become commonplace researchers have confirmed that not all visual properties are processed in an equal manner. Behavioral results predicted this \citep{Li2002-ji,Moray1959-fn} and one of the easiest operationalized tasks in which to observe this is during search \citep{Wolfe1994-ew}. In a search task, an observer will be cued in advance to reveal the location of a particular item, e.g. by pointing to it, or to assess its presence, e.g. by pressing a button to indicate that it is present. The item will be hidden among a set of distractors whose properties determine the difficulty of the task. Such tasks are trivial when the target stimulus differs from the distractors along certain key dimensions: color, orientation, spatial frequency. Trivial, in this case, means that the processing required to solve the task occurs in parallel and the solution is found by the observer in “one step”, so to speak. This matches with experiments in the early visual system which show that neurons are characterized by their tuning to orientation, spatial frequency, and to particular regions of visual space \citep{Barlow1957-by,Hubel1962-pn}. The tiling of these neurons across retinotopic space results in visual field maps, of which there are more than a dozen identified in human visual cortex \citep{Wade2002-tt,Wandell2007-pr,Wandell2011-td}. In these simple search tasks, it appears that early visual cortex performs computations largely in parallel across these maps. Differences in the strength of signals across these maps, for each feature, can then result in ‘pop out’ of the relevant stimulus (Nothdurft, 1993; A. Treisman, 1985). 

Difficult search tasks involve conjunctions of stimulus properties (Egeth, Virzi, & Garbart, 1984) which require attention to be directed in a serial manner to each item (A. M. Treisman & Gelade, 1980). The results from search experiments match the findings of early physiology experiments (Hubel & Wiesel, 1959, 1968) which showed that the early visual cortex starts by processing in parallel the same simple features which ‘pop out’ during search. Sharp differences in these features drive bottom-up attention, or salience, in which features that differ from their neighbors can cause an observer to orient to them. In contrast to this parallel stage, higher areas in visual cortex process complex stimuli such as objects, but this processing does not appear to occur in parallel for multiple objects, instead objects trade off for representation according to the focus of attention (Desimone, 1998). Instead of the repetitive receptive field structure in early visual cortex these areas have large receptive fields and retinotopic bias according to experience, for example there is a foveal bias in areas selective for faces and a peripheral bias in areas that are selective for locations (Levy, Hasson, Avidan, Hendler, & Malach, 2001). Importantly, the physiological effects of selective attention are not isolated to any one of these areas. 

Our understanding of the neural implementation has advanced dramatically thanks to recordings of neuronal populations from animal models. But animal models have also held back research in important ways. In general it is not possible to probe an animals memory about unattended stimuli, rendering many classic psychophysics paradigms difficult to interpret. Whether the implementation of attention in a primate trained over months is similar to that in a human trained over the course of minutes is an unknown at the moment. Recently, mice have been shown to be able to exhibit selective attention (McBride, Lee, & Callaway, 2019; Wang & Krauzlis, 2018). This is promising for understanding the role of different brain areas in attentional selection, but also concerning. The mice in these studies exhibit a bias which resembles selective attention, but they also have high lapse rates compared to human observers. One explanation is that mice continue to explore the experiment space (Pisupati, Chartarifsky-Lynn, Khanal, & Churchland, 2019) whereas humans who learn from rules do not. Correctly taking these differences into account, perhaps by linking animal research directly to parallel human research, has a good chance of overcoming these issues. 

Despite the many results showing that sensory representations change during directed attention researchers have yet to converge on a consistent algorithm which implements selection. Instead, a number of competing theories exist for how sensory selection might proceed, including changes in sensitivity (Reynolds et al., 2000; Serences & Boynton, 2007; Snyder, Yu, & Smith, 2018; Treue & Martínez Trujillo, 1999), shifts in feature selectivity (Çukur, Nishimoto, Huth, & Gallant, 2013; David, Hayden, Mazer, & Gallant, 2008; Kastner, De Weerd, Desimone, & Ungerleider, 1998; Klein et al., 2014; Spitzer et al., 1988; Womelsdorf et al., 2006; Womelsdorf, Anton-Erxleben, & Treue, 2008), increases in baseline response (Buracas & Boynton, 2007; Chen & Seidemann, 2012; Fang, Boyaci, Kersten, & Murray, 2008; Kastner, Pinsk, De Weerd, Desimone, & Ungerleider, 1999; X. Li, Lu, Tjan, Dosher, & Chu, 2008), and changes in the structure of stimulus-driven and noise correlations (Cohen & Maunsell, 2009, 2011; Mitchell et al., 2009; Ruff & Cohen, 2016; Verhoef & Maunsell, 2017). All of these changes affect the information present in sensory cortex to different degrees. Depending on the overlap and correlation between stimulus representations enhancement to one stimulus may results in loss of information for another. In contrast with these theories, flexible readout which leaves stimulus representation unchanged (Birman & Gardner, n.d.) may be a useful alternative when behavior needs to remain adaptable. Deciphering the balance of sensory change against change in readout requires models which can quantify the extent to which neural changes result in behavioral changes. 

\subsection{Computational linking models}

Measurements of the neural effects of selective attention are not sufficient to understand its implementation, they must be linked correctly to behavior. To reconcile changes in cortical activity with behavior, cognitive neuroscientists can link measurements of neuronal activity and behavior using computational linking models. One assumption underlying much of cognitive neuroscience is that when we make a measurement of cortical activity, we are seeing the same signals that the brain uses the solve sensory decision making. This is only an assumption; it is possible that sensory decision making (and other forms of neural processing) are based on subsets of signals, or population codes, which remain harder to measure. To avoid making errors in inference it is important to make these hypotheses (or assumptions) about implementation explicit in a form which can be tested against other possible hypotheses. One way to do this is to build computational models which lay out the steps from sensory signal to sensory decision. We refer to these as “linking models”, as they link together perceptual measurements and cortical ones. 

Recent work has begun to explore computational models which link measures of behavior to neuronal recordings, i.e. building linking models (Barlow, 1972; Brindley, 1960; Cohen & Maunsell, 2010; Cook & Maunsell, 2002; Newsome, Britten, & Movshon, 1989; Pestilli et al., 2011). These experiments have shown that sensory responses are enhanced during directed attention while noise unrelated to the stimulus is altered and reduced \citep{Ecker2016-ro,Rabinowitz2015-uz,Snyder2018-yr}, which may be sufficient to cause neural activity to ``align'' to the dimension of readout \citep{Ruff2018-yx}. In human research similar tasks have been shown to cause estimates of receptive fields to shift \citep{Klein2014-oe} which might be sufficient to enhance spatial sensitivity at attended locations \citep{Klein2016-ox,Vo2017-oi} consistent with a response gain occurring in earlier layers of the visual system \citep{Baruch2014-gy,Miconi2016-ip}.
 
In a recent paper Pestilli et al. (2011) found that a simple linking model of response gain during spatial attention was quantitatively insufficient to explain behavior. In that work, the authors found that a different form of readout was necessary to explain how the seemingly small changes in sensory representation could lead to large improvements in perceptual sensitivity. In non-human primates, linking models have been used to hypothesize about the possible roles of single neurons or populations in sensory decision making \citep{Newsome1989-fr}. Similar results in humans have implicated early visual cortex as a source of information about contrast discrimination \citep{Boynton1999-jd}. Finally, hypotheses about how sensory selection might alter the population code in cortex have been tested with linking models as well \citep{Cohen2011-pa}. Although none of these examples involve causal manipulations (although they often depend on the results of such research) they nevertheless have considerable value to the field because they make explicit their assumptions about how sensory decision making proceeds.

Linking models have the additional advantage that they allow researchers to begin to speculate about why sensory selection might be implemented in different ways. Depending on where sensory information is represented selection may have to occur in different ways: scene gist, which survives inattention \citep{Li2002-ji,Peelen2009-us}, may require very different kinds of selection to suppress compared to irrelevant spatial information \citep{Pestilli2011-gi}. Task demands likely also change the form of selection, as well as the computational costs associated with different cortical implementations (Gardner, 2019). Changing sensory representations during selective attention may reflect a computationally efficient solution where the visual system is discarding, and therefore not fully processing, stimuli that are irrelevant to behavior. In contrast, flexible mechanisms which compute sensory decisions in a context-dependent manner \citep{Mante2013-tn} may require resources to represent and process stimuli that may not ultimately be behaviorally relevant. 

\section{Dissertation scope}

In this chapter, I have summarized literature relevant to our current understanding of selective visual attention. The goal of this dissertation is to test two of the aspects of sensory selection which remain poorly understood.

\subsection{Aim 1: Comparing spatial and feature-based attention on a shared perceptual metric}

Although we know that different visual features are processed in different ways, this does not necessarily mean that selection by these features requires different computational resources. It is also unclear whether selection by one feature, such as space, might be more efficient or strong than selection by a different feature such as color.

\textbf{In Chapter 2, I address this aim by building a psychophysics experiment with which I compare the strength of spatial and feature-based attention on a common metric.} I demonstrate in these experiments that the sensitivity (i.e. the variability in response error) is constant across different forms of selection, but that selection by feature is more likely to result in response bias from overlapped distractors. 

\subsection{Aim 2: Using linking models to compare implementations of selective visual attention}

The prevailing view is that attention is implemented by modifications of sensory responses. But without a computational linking model it is impossible to know whether these sensory changes are the only changes occurring during selective visual attention. 

\textbf{In Chapter 3, I build a quantitative framework for two features of motion visibility.} Contrast and motion coherence both control the visibility of motion and their representations in human visual cortex are similar. This makes them excellent tools to study whether sensory change alone can account for the behavioral effects of attention. In this chapter, I measure and quantify how these features are represented to lay the ground work for a linking model. This chapter is published in the \textit{Journal of Neurophysiology} \citep{Birman2018-sp}.

\textbf{In Chapter 4, I use a linking model to show that sensory change is insufficient to account for all the behavioral effects of attention.} I first validate that a linking of motion visibility can be constructed, based on the quantitative framework developed in Chapter 3. Then, using the validated linking model I show that the sensory change occurring during selective visual attention is insufficient to account for behavioral changes -- a flexible readout must be a necessary component. This chapter is under review in \textit{Nature Communications} \citep{Birman_undated-vk}.
