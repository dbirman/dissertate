% Stanford University PhD thesis style -- modifications to the report style
% This is unofficial so you should always double check against the
% Registrar's office rules
% See http://library.stanford.edu/research/bibliography-management/latex-and-bibtex
% 
% Example of use below
% See the suthesis-2e.sty file for documentation
%
\documentclass{report}
\usepackage{suthesis-2e}
\dept{Psychology}

\begin{document}
\title{Flexible Readout\\
            from Human Visual Cortex}
\author{Daniel Birman}
\principaladviser{Justin L. Gardner}
\firstreader{Kalanit Grill-Spector}
\secondreader{Anthony Norcia}
\thirdreader{Tirin Moore} %if needed
% \fourthreader{Severus Snape} %if needed
 
\beforepreface
\prefacesection{Preface}
We consciously perceive only small amounts of the sensory world. Where does the brain filter out the irrelevant inputs? 
\prefacesection{Acknowledgments}
todo...
\afterpreface

\chapter{Introduction}

\section{Background}
In early attention research, observers used introspection to evaluate how their sensory experiences changed when they focused in various ways. These early researchers found that attention could change the conscious perception of a stimulus, making it more intense or clear \cite{Helmholtz1924-rl,James1981-cj,Kuelpe1902-qz,Titchener1908-bx}. These intuitions were not without merit, as spatial attention to some stimuli does appear to lead to a change in the appearance of a stimulus \cite{Carrasco2018-sb}. But without concepts to tackle the ideas of mechanisms and computation these authors never succeeded in making headway into how attention might be implemented in the human brain. 

Introspection yielded in the middle of the 20th century to more quantitative approaches in which attention, broadly defined, was operationalized into specific tasks. This allowed researchers to begin to make claims about the algorithm of attention. In particular, psychophysicists in the mid-20th century used reports about unattended stimuli to understand how information was selected. Perhaps the earliest example of this is a set of experiments in which observers were asked to echo speech from one ear at a time \cite{Cherry1953-as}. The transition from introspection to ‘objective’ measurement can be highlighted in the way these experiments were introduced, quoting from Cherry (1953): “the ‘subject’ under test (the listener) is regarded as a transducer whose responses are observed when various stimuli are applied, whereas his subjective impressions are taken to be of minor importance”. This approach was effective. The field made an abrupt move toward understanding sensory systems as processing units. Any given stimulus could be processed in a serial or parallel manner and processing might go to ‘completion’ or be halted at a certain point. Measuring the knowledge of a subject about an attended or ignored stimulus therefore assesses the extent of processing which has occurred for that stimulus. For example, for the echoing task described above, an observer might be asked whether they had retained knowledge about an un-echoed (and therefore, unattended) voice. If they could recall simple low-level features, e.g. that the speaker had a higher-pitched voice, then a researcher could conclude that parallel processing  of both streams had occurred up to pitch processing. They could then also conclude that processing of the ignored stimulus had then been halted, before the point of complete semantic understanding. Such results led researchers to begin to distinguish between stages of processing: an early parallel stage in which incoming sensory information is processed without immediate limits in capacity, and a second limited capacity serial stage from which complex decisions could be made.

Several early theories of attention focused on when selection occurs relative to the parallel and serial stages of processing. These theories fell out of the observations made above, that some features of auditory, as well as other sensory, stimuli are available for decision making regardless of the observers focus while other features must be attended to be available. To reconcile these facts researchers suggested a bottleneck and proposed that this was the mechanistic implementation of selective attention. The first such modern theory was Broadbent’s Filter theory \cite{Broadbent1958-ny}, which included an early bottleneck. In Filter theory visual information is processed in parallel until low-level features (location, intensity, frequency) are resolved. At this point, parallel processing gives way to a serial ‘complete’ processing of object identity, form, etc. The alternative theory, late selection, suggests that processing occurs up to semantics in an unconscious parallel manner \cite{Deutsch1963-ac}. An example best illustrates the distinctions in these theories. Again, if an observer is echoing one speaker while ignoring a second, a late selection account predicts that a substantial amount of information is nevertheless available about the ignored voice. This is because late selection predicts that a semantic-level processing of speech will occur regardless of sensory selection, leaving high-level information available to an observer. Evidence for this comes from experiments in which observers orient to highly salient, but also very high-level features, such as their own name, even when focused on other tasks \cite{Moray1959-fn}. This suggests that not also features are processed in identical ways. Selection theories were expanded upon, based on this suggestion that features might differ in their form of processing and not be consistently selected out in one step. Instead, it was suggested that features might be attenuated \cite{Treisman1960-qs} or that selection might be limited by variable capacity at different stages of processing \cite{Kahneman1973-af}.

Selective attention has been most heavily studied in the visual domain, in part because of the close relationship between selecting information in the world and orienting of the eyes. Spatial orienting can occur both in an overt movement of the eyes, but also covertly without eye movements. This covert attention accelerates responses \cite{Eriksen1972-qj} (Posner, Snyder, & Davidson, 1980) and both improves detection performance and increases discrimination sensitivity \cite{Carrasco2011-xp}. Cues about important locations can both be imposed externally on an observer (Posner, 1980) or the result of internal guiding of attention toward the cued location. Covert attention can also be both spatial, related to eye movements, or featural \cite{Rossi1995-wa} as described below for search tasks. Because of this close relationship between selection (due to eye movements) and vision, researchers have focused in particular on the implementation of selective attention in the visual system. 

\section{Implementations of selective attention}

Selective attention is a balancing act for the brain, which must weigh the possibility of needing unattended information against the strength of sensory selection. In an early selection theory, information is being thrown out before complete processing -- which could be potentially disadvantageous if that stimulus later becomes important for behavior (Mack & Rock, 1998). Late 20th century research on attention was well aware of this fact, and researchers showed that in different contexts sensory selection can be more or less complete at filtering out irrelevant information. In a few particularly dramatic demonstrations (Haines, 1991; Mack & Rock, 1998; Neisser, 1979; Simons & Chabris, 1999) observers can be shown to entirely lose access to otherwise highly salient information, but only when performing a task with significant cognitive load (Lavie, 2005; Lavie, Hirst, de Fockert, & Viding, 2004; Rees, Frith, & Lavie, 1997). In vision, like in audition (Moray, 1959), some visual information is processed no matter what, as scene gist can survive inattention, perceptually (F. F. Li, VanRullen, Koch, & Perona, 2002) and as decodable information from measurements of BOLD signal in visual cortex (Peelen, Fei-Fei, & Kastner, 2009). These examples highlight the delicate balance that the brain must maintain: with limited resources how should the brain allocate these to efficiently sample the sensory world? Early research on the implementation of attention put this question to the side while first exploring the possible effects of attention on sensory processing. In apparent contradiction with the specific bottlenecks proposed by early and late selection theories, physiologists found that behavioral selection results in neural changes throughout the brain: in early visual cortex (where, presumably ‘parallel’ processing occurs), higher visual cortex, and in non-sensory regions. Reconciling these observations with the behavioral effects of selective attention has become a major goal of cognitive neuroscience. 

Modern neuroscience now deploys a multitude of neural recording techniques to understand how behavioral selection might be implemented by the brain. Both at a coarse scale in humans and at the level of individual neurons in primates and, very recently, rodents. In humans and non-human primates attention has been shown to alter the response gain of neurons in the visual system, including in the LGN  (O’Connor, Fukui, Pinsk, & Kastner, 2002), in V1 (Motter, 1993), V2 (Buffalo, Fries, Landman, Liang, & Desimone, 2010; Luck, Chelazzi, Hillyard, & Desimone, 1997; Motter, 1993), V3 (Liu, Larsson, & Carrasco, 2007; Pestilli, Carrasco, Heeger, & Gardner, 2011; Saenz, Buracas, & Boynton, 2002; Silver, Ress, & Heeger, 2007), V4 (Buffalo et al., 2010; Connor, Gallant, Preddie, & Van Essen, 1996; Luck et al., 1997; McAdams & Maunsell, 1999; Moran & Desimone, 1985; Motter, 1993; Reynolds, Pasternak, & Desimone, 2000; Spitzer, Desimone, & Moran, 1988), V3A (Serences & Boynton, 2007),  MT (Beauchamp, Cox, & DeYoe, 1997; O’Craven, Rosen, Kwong, Treisman, & Savoy, 1997; Saenz et al., 2002; Seidemann, Poirson, Wandell, & Newsome, 1999; Serences & Boynton, 2007; Treue & Martínez Trujillo, 1999; Treue & Maunsell, 1996) and MST (O’Craven et al., 1997; Treue & Maunsell, 1996), and in IT cortex (Chelazzi, Duncan, Miller, & Desimone, 1998; Moran & Desimone, 1985). Using BOLD imaging these changes can be observed simultaneously throughout almost all of early visual cortex (Liu et al., 2007; Pestilli et al., 2011; Saenz et al., 2002; Silver et al., 2007) and ventral temporal cortex (Baldauf & Desimone, 2014). Although the neurochemical mechanisms of these effects remain unclear, muscarinic acetylcholine receptors are known to play a key role (Herrero et al., 2008). Changes to sensory representations occur both for spatial attention tasks (Klein, Harvey, & Dumoulin, 2014; McAdams & Maunsell, 1999; Mitchell, Sundberg, & Reynolds, 2009; Pestilli et al., 2011; Womelsdorf, Anton-Erxleben, Pieper, & Treue, 2006) and feature-based attention tasks (Baldauf & Desimone, 2014; Harel, Kravitz, & Baker, 2014; Huk & Heeger, 2000; Jehee, Brady, & Tong, 2011; Saenz et al., 2002; Sàenz, Buraĉas, & Boynton, 2003; Serences & Boynton, 2007; Treue & Martínez Trujillo, 1999).

As measurements of physiology have become commonplace researchers have also confirmed that not all visual properties are processed in an equal manner. Behavioral results predicted this (F. F. Li et al., 2002; Moray, 1959) and one of the easiest operationalized tasks in which to observe this is during search (Wolfe, 1994). In a search task, an observer will be cued in advance to reveal the location of a particular item, e.g. by pointing to it, or to assess its presence, e.g. by pressing a button to indicate that it is present. The item will be hidden among a set of distractors whose properties determine the difficulty of the task. Such tasks are trivial when the target stimulus differs from the distractors along certain key dimensions: color, orientation, spatial frequency. Trivial, in this case, means that the processing required to solve the task occurs in parallel and the solution is found by the observer in “one step”, so to speak. This matches with experiments in the early visual system which show that neurons are characterized by their tuning to orientation, spatial frequency, and to particular regions of visual space (Barlow, Fitzhugh, & Kuffler, 1957; Hubel & Wiesel, 1962). The tiling of these neurons across retinotopic space results in visual field maps, of which there are more than a dozen identified in human visual cortex (Wade, Brewer, Rieger, & Wandell, 2002; Wandell, Dumoulin, & Brewer, 2007; Wandell & Winawer, 2011). In these simple search tasks, it appears that early visual cortex performs computations largely in parallel across these maps. Differences in the strength of signals across these maps, for each feature, can then result in ‘pop out’ of the relevant stimulus (Nothdurft, 1993; A. Treisman, 1985). 

Difficult search tasks involve conjunctions of stimulus properties (Egeth, Virzi, & Garbart, 1984) which require attention to be directed in a serial manner to each item (A. M. Treisman & Gelade, 1980). The results from search experiments match the findings of early physiology experiments (Hubel & Wiesel, 1959, 1968) which showed that the early visual cortex starts by processing in parallel the same simple features which ‘pop out’ during search. Sharp differences in these features drive bottom-up attention, or salience, in which features that differ from their neighbors can cause an observer to orient to them. In contrast to this parallel stage, higher areas in visual cortex process complex stimuli such as objects, but this processing does not appear to occur in parallel for multiple objects, instead objects trade off for representation according to the focus of attention (Desimone, 1998). Instead of the repetitive receptive field structure in early visual cortex these areas have large receptive fields and retinotopic bias according to experience, for example there is a foveal bias in areas selective for faces and a peripheral bias in areas that are selective for locations (Levy, Hasson, Avidan, Hendler, & Malach, 2001). Importantly, the physiological effects of selective attention are not isolated to any one of these areas. 

Our understanding of the neural implementation has advanced dramatically thanks to recordings of neuronal populations from animal models. But animal models have also held back research in important ways. In general it is not possible to probe an animals memory about unattended stimuli, rendering many classic psychophysics paradigms difficult to interpret. Whether the implementation of attention in a primate trained over months is similar to that in a human trained over the course of minutes is an unknown at the moment. Recently, mice have been shown to be able to exhibit selective attention (McBride, Lee, & Callaway, 2019; Wang & Krauzlis, 2018). This is promising for understanding the role of different brain areas in attentional selection, but also concerning. The mice in these studies exhibit a bias which resembles selective attention, but they also have high lapse rates compared to human observers. One explanation is that mice continue to explore the experiment space (Pisupati, Chartarifsky-Lynn, Khanal, & Churchland, 2019) whereas humans who learn from rules do not. Correctly taking these differences into account, perhaps by linking animal research directly to parallel human research, has a good chance of overcoming these issues. 

Despite the many results showing that sensory representations change during directed attention researchers have yet to converge on a consistent algorithm which implements selection. Instead, a number of competing theories exist for how sensory selection might proceed, including changes in sensitivity (Reynolds et al., 2000; Serences & Boynton, 2007; Snyder, Yu, & Smith, 2018; Treue & Martínez Trujillo, 1999), shifts in feature selectivity (Çukur, Nishimoto, Huth, & Gallant, 2013; David, Hayden, Mazer, & Gallant, 2008; Kastner, De Weerd, Desimone, & Ungerleider, 1998; Klein et al., 2014; Spitzer et al., 1988; Womelsdorf et al., 2006; Womelsdorf, Anton-Erxleben, & Treue, 2008), increases in baseline response (Buracas & Boynton, 2007; Chen & Seidemann, 2012; Fang, Boyaci, Kersten, & Murray, 2008; Kastner, Pinsk, De Weerd, Desimone, & Ungerleider, 1999; X. Li, Lu, Tjan, Dosher, & Chu, 2008), and changes in the structure of stimulus-driven and noise correlations (Cohen & Maunsell, 2009, 2011; Mitchell et al., 2009; Ruff & Cohen, 2016; Verhoef & Maunsell, 2017). All of these changes affect the information present in sensory cortex to different degrees. Depending on the overlap and correlation between stimulus representations enhancement to one stimulus may results in loss of information for another. In contrast with these theories, flexible readout which leaves stimulus representation unchanged (Birman & Gardner, n.d.) may be a useful alternative when behavior needs to remain adaptable. Deciphering the balance of sensory change against change in readout requires models which can quantify the extent to which neural changes result in behavioral changes. 

\subsection{Computational linking models}

Measurements of the neural effects of selective attention are not sufficient to understand its implementation, they must be linked correctly to behavior. To reconcile changes in cortical activity with behavior, cognitive neuroscientists can link measurements of neuronal activity and behavior using computational linking models. One assumption underlying much of cognitive neuroscience is that when we make a measurement of cortical activity, we are seeing the same signals that the brain uses the solve sensory decision making. This is only an assumption; it is possible that sensory decision making (and other forms of neural processing) are based on subsets of signals, or population codes, which remain harder to measure. To avoid making errors in inference it is important to make these hypotheses (or assumptions) about implementation explicit in a form which can be tested against other possible hypotheses. One way to do this is to build computational models which lay out the steps from sensory signal to sensory decision. We refer to these as “linking models”, as they link together perceptual measurements and cortical ones. 

Recent work has begun to explore computational models which link measures of behavior to neuronal recordings, i.e. building linking models (Barlow, 1972; Brindley, 1960; Cohen & Maunsell, 2010; Cook & Maunsell, 2002; Newsome, Britten, & Movshon, 1989; Pestilli et al., 2011). These experiments have shown that sensory responses are enhanced during directed attention while noise unrelated to the stimulus is altered and reduced (Ecker, Denfield, Bethge, & Tolias, 2016; Rabinowitz, Goris, Cohen, & Simoncelli, 2015; Snyder et al., 2018), which may be sufficient to cause neural activity to “align” to the dimension of readout (Ruff, Ni, & Cohen, 2018). In human research similar tasks have been shown to cause estimates of receptive fields to shift (Klein et al., 2014) which might be sufficient to enhance spatial sensitivity at attended locations (Klein, Paffen, Pas, & Dumoulin, 2016; Vo, Sprague, & Serences, 2017) consistent with a response gain occurring in earlier layers of the visual system (Baruch & Yeshurun, 2014; Miconi & VanRullen, 2016).
 
In recent work, we have used computational linking models to show that change in sensory representation cannot be the sole implementation of attention during selective attention to motion visibility (Birman & Gardner, n.d., 2018). Using BOLD imaging we carefully measured how retinotopic visual cortex changed when observers were exposed to changes in the visibility of random dot motion patches. Using a perceptual discrimination task, we measured human perceptual sensitivity to these same stimulus properties. We then created a series of hypotheses about how sensory decision making might be solved in such a task and made our hypotheses explicit by instantiating each as a candidate linking model. For example, when asked to report about one stimulus feature such as contrast, but not about another such as the coherence, the brain might resort to enhancing and suppressing their respective sensory representations. Alternatively, the brain could leave sensory representations alone while changing downstream readouts. In these experiments, we found that models which had both a small amount of sensory change and a downstream readout best explained the data. Without the linking model, it is possible that we would have concluded that sensory changes alone implement such a context-dependent behavior, not knowing that they were quantitatively insufficient in our measurements.  

Other successful recent examples of linking models highlight the value of the approach. In a recent paper Pestilli et al. (Pestilli et al., 2011) also found that a simplistic linking model of response gain during spatial attention was quantitatively insufficient to explain behavior. In that work, the authors found that a different form of readout was necessary to explain how the seemingly small changes in sensory representation could lead to large improvements in perceptual sensitivity. In non-human primates, linking models have been used to hypothesize about the possible roles of single neurons or populations in sensory decision making (Newsome et al., 1989). Similar results in humans have implicated early visual cortex as a source of information about contrast discrimination (Boynton, Demb, Glover, & Heeger, 1999). Finally, hypotheses about how sensory selection might alter the population code in cortex have been tested with linking models as well (Cohen & Maunsell, 2011). Although none of these examples involve causal manipulations (although they often depend on the results of such research) they nevertheless have considerable value to the field because they make explicit their assumptions about how sensory decision making proceeds.

Linking models have the additional advantage that they allow researchers to begin to speculate about why sensory selection might be implemented in different ways. Depending on where sensory information is represented selection may have to occur in different ways: scene gist, which survives inattention (F. F. Li et al., 2002; Peelen et al., 2009), may require very different kinds of selection to suppress compared to irrelevant spatial information (Pestilli et al., 2011). Task demands likely also change the form of selection, as well as the computational costs associated with different cortical implementations (Gardner, 2019). Changing sensory representations during selective attention may reflect a computationally efficient solution where the visual system is discarding, and therefore not fully processing, stimuli that are irrelevant to behavior. In contrast, flexible mechanisms which compute sensory decisions in a context-dependent manner (Mante, Sussillo, Shenoy, & Newsome, 2013) may require resources to represent and process stimuli that may not ultimately be behaviorally relevant. 

\section{Roadmap}

In my research projects I have focused on studying whether measurements of change in sensory representations, due to attention, are of the right magnitude to match changes in behavioral performance. I focus on two complimentary implementations of attention: change in sensory representation during selective attention and flexibility in context-dependent readout. To study this I build linking models, computational models that connect physiological measurements directly to perceptual performance. The first two projects are together a study of how the readout of motion visibility occurs. We found that sensory change does not fully account for behavioral changes in these tasks. Instead, flexible readout, is an essential component of sensory selection. Following on that result, I have now been looking into whether this finding is robust for sensory selection using features such as color, motion direction, or spatial location. At the time of my defense (August 1st, 2019), I expect to have completed Project 1 and 2 which will each result in a peer-reviewed publication. Project 3 will be ongoing, with preliminary data from the behavioral experiment. 
\subsection{A quantitative framework for motion visibility in human visual cortex}
Before being able to ask about readout and decision-making regarding motion visibility I found that there were no existing comprehensive models of the human visual cortex response to these parameters, and set out to build that dataset. To do this we recorded the hemodynamic response to patches of dots with varying contrast, coherence, and duration. Using those measurements, we constrained a model of the population response functions, i.e. an estimate of the neural activity within different cortical visual areas. We found that early visual cortex, in particular V1-V4 showed strong parametric modulation by contrast. Coherence on the other hand caused the strongest parametric modulation in MT and V3A. But across observers we found that for the most part every cortical area showed some sensitivity to both motion visibility features. This last point was critical for us to learn because it suggests that a readout process that wants to separately decode contrast or coherence but sees the generic activity of all of visual cortex, would need to deal with these overlapping signals in some way. In this project we were also able to demonstrate the nonlinearity of the BOLD signal response in early visual cortex, confirming other results showing that different cortical areas may differ in their transient and sustained components of their response (Boynton, Engel, Glover, & Heeger, 1996; Boynton, Engel, & Heeger, 2012; Huettel & McCarthy, 2000; Ogawa et al., 2000; Stigliani, Jeska, & Grill-Spector, 2017). 
\subsection{A flexible readout mechanism of human sensory representations}
To build a linking model of motion visibility we collected a behavioral dataset in which human observers judged the visibility of patches of dots and then compared various models of how readout might occur to enable that behavior. We found that human observers could flexibly switch between reporting about the two features we studied, contrast and coherence, with little to no interference from the other. We tested whether this behavior was best explained by a readout process that involves changing sensory representations (e.g. through enhancement or suppression) or one that leaves representations the same and flexibly changes readout.
We used the quantitative framework described in 1a to build a linking model of physiology and perception, based on the idea that the readout process is a pooling of weighted signals from different cortical visual areas. We then collected a new physiological dataset while observers performed the different behavioral tasks in the scanner to see whether enhancement or suppression would show up and whether these effects would be of the right magnitude to support behavior. On the contrary we found that the effects of directed viewing on enhancement and suppression are far too small -- instead a flexible readout process must be operating to support these changes.
\subsection{Comparing spatial and feature-based attention}
Visual cortex does not necessarily represent spatial location in a computationally different way from features such as color or orientation. It’s possible that retinotopy, although dramatic when measured with a topographical tool such as BOLD imaging, is actually treated computationally by the brain in a similar manner to other neural codes. As described in detail above, spatial attention leads to receptive field shifts when measured with BOLD imaging (Klein et al., 2014; Vo et al., 2017). Feature-based attention appears to also result in an analogous shift in selectivity, but in feature-space (Çukur et al., 2013). Our goal in this project was to understand whether these shifts are similar in nature and whether they account for changes in perceptual sensitivity during selective attention. Our approach was to extend our earlier results on motion visibility to spatial attention and to additional features (color and motion direction) by again building a linking model of BOLD measurement to perceptual sensitivity.
We designed a task to directly compare selection by space to selection by either color or motion direction, based on similar early work on feature-based attention (Saenz et al., 2002; Sàenz et al., 2003). Observers in our task are shown two pairs of overlapped dot patches to the left and right of fixation. On a trial, observers report about the color of a single patch of dots but can be cued to all four patches (no cue), or to remember the two patches on one side or the two with matched features (e.g. the two patches moving upwards). If observers are more perceptually sensitive when they are cued to remember, for example, the two patches on the same side, then we might suspect that spatial selection improves sensory encoding in some way. This data therefore allows a comparison between a spatial form of cueing an a feature-based one. In the preliminary phase of this project, we have collected behavioral evidence showing that spatial and feature-based attention result in a similar improvement in perceptual sensitivity compared to control conditions. Nevertheless, spatial location is privileged in some ways: having to select one feature while ignoring a spatially overlapped distractor leads to a bias where observers often confuse the two spatially overlapping stimuli. This finding is similar to other results in which feature-based attention sometimes leads to enhancements of irrelevant information (Jehee et al., 2011), perhaps due to binding of stimulus properties into objects. Once the bias is accounted for we find no evidence for differences in perceptual sensitivity based on spatial or feature-based selection. These results are consistent with a view of selection that is blind to the special topographic organization of space in sensory cortex. For visual cortex, all features are the same. 
In future work in this project we will collect measurements of BOLD signal in human visual cortex while observers perform the task described. Our expectation is that a linking model can be built based on the sensory changes during selective attention to account for a portion of the improvement in perceptual sensitivity which occurs. Our prediction is that these sensory changes will be of a similar magnitude for both feature-based and spatial selection, in parallel with the similar behavioral results thus far obtained.

\chapter{Aim 1: A quantitative framework for motion visibility in human cortex}
\section{Introduction}
Much of the neural basis of perception has been revealed by manipulations that control the visibility of motion stimuli. For example, global motion direction of random-dot stimuli is made less visible by decreasing motion coherence, i.e., the percentage of dots moving in the same direction. At lower visibility levels, small changes in cortical signals manifest in measurable behavioral effects, thus documenting direct links between cortical physiology and perception (Britten et al. 1992; Newsome et al. 1989) and uncovering neural signals supporting evidence accumulation (Huk and Shadlen 2005; Katz et al. 2016; Roitman and Shadlen 2002; Shadlen et al. 1996; Shadlen and Newsome 2001). Making stimuli brief also renders them less visible, aiding, for example, the study of information integration across eye movements (Melcher and Morrone 2003). Increasing image contrast, the average difference between bright and dark (Bex and Makous 2002), makes stimuli more visible and cortical responses monotonically larger allowing links to be made between cortical response and perception (Boynton et al. 1999; Ress et al. 2000; Ress and Heeger 2003), disambiguating mechanisms for spatial attention (Carrasco et al. 2000; Hara and Gardner 2014; Hara et al. 2014; Pestilli et al. 2011), uncovering neural correlates of conscious perception (Lumer et al. 1998; Wunderlich et al. 2005), and revealing the effects of putative priors (Stocker and Simoncelli 2006; Vintch and Gardner 2014). While each of these manipulations has been used extensively in the human perceptual literature, they can have greatly different effects on human neural response. Given the central importance of motion visibility, a quantitative model of response across human visual cortex is required to provide a framework for interpreting and building upon these various findings.

Such a population response model must quantitatively account for the shape of the relationship between motion visibility and cortical response. The response function for contrast has been characterized as a sigmoidal function for measurements in single units (Albrecht and Hamilton 1982; Sclar et al. 1990) and populations (Avidan et al. 2002; Boynton et al. 1996, 1999; Gardner et al. 2005; Logothetis et al. 2001; Olman et al. 2004; Tootell and Taylor 1995; Tootell et al. 1998). Increasing motion coherence typically results in linear increases in response (Aspell et al. 2005; Britten et al. 1993; Händel et al. 2007; Rees et al. 2000; Simoncelli and Heeger 1998) although this may depend on the exact stimulus parameters (Ajina et al. 2015).

A population response model must also quantify the variable sensitivity to visibility parameters across cortical areas. The earliest cortical areas have a larger dynamic range for contrast compared with later areas which are more invariant (Avidan et al. 2002; Cheng et al. 1994; Rolls and Baylis 1986; Sclar et al. 1990). Less is known about motion coherence sensitivity except that the neural response to coherent compared with incoherent motion or blank evokes a large response in the human middle temporal area (hMT+, referred to as MT) with some sensitivity reported in earlier visual cortical areas (Ajina et al. 2015; Costagli et al. 2014; Dupont et al. 1994; Heeger et al. 1999; Tootell et al. 1995; Watson et al. 1993; Zeki et al. 1991) and parietal and ventral regions (Braddick et al. 2001).

Finally this model must account for stimulus duration effects. Hemodynamic responses to visual stimuli are approximately temporally linear except when durations (Boynton et al. 1996, 2012) or interstimulus intervals (Huettel and McCarthy 2000) are brief. The divergence from linearity may differ across cortical areas (Birn et al. 2001) and motion-sensitive regions may be most sensitive to transient changes (Stigliani et al. 2017).

Here we measured blood-oxygen-level dependent (BOLD) (Ogawa et al. 1990) response in human observers to a large range of contrast, coherence and duration of motion stimuli, and built a quantitative model linking these visibility properties with physiological response in retinotopically defined visual areas. Sensitivity to these parameters varied significantly across areas, although all were sensitivity to both contrast and coherence without interaction. While perceptual experiments have often used different means of affecting visibility interchangeably our results provide a reference model that underscores the differences in response to each manipulation of visibility across cortical areas, thus providing a quantifiable way to interpret experiments that link cortical response to perception.
\section{Methods}
Observers.
In total, 11 observers (8 female, 3 male; mean age 26 y; age range 19–36 y) were subjects for the experiments. All observers except one (who was an author) were naïve to the intent of the experiments. Observers were scanned three times, in 2 two-hour sessions of the experiment and a one hour retinotopy session. Procedures were approved in advance by the Stanford Institutional Review Board on human participants research and all observers gave prior written informed consent before they participated in the experiment. When necessary, observers wore corrective lenses to correct their vision to normal.

Hardware setup for stimulus and task control.
Visual stimuli were generated using MATLAB (The MathWorks) and MGL (Gardner et al. 2018b) (http://gru.stanford.edu/mgl). Stimuli were backprojected via an Eiki LC-WUL100L projector (resolution of 1,900×1,200, refresh rate of 100 Hz) onto an acrylic sheet mounted inside the scanner bore near the head coil. Visual stimuli were viewed through a mirror mounted on the head coil and responses were collected via an MRI-compatible button box. Output luminance was measured with a PR650 spectrometer (Photo Research) and a neutral density filter used to set the average screen luminance to 300 cd/m2. The gamma table was then dynamically adjusted at the beginning of each trial to linearize the luminance display such that the full 10-bit output resolution of the gamma table could be used to display the maximum contrast needed. Other sources of light were minimized during scanning.

Eye tracking.
Prior to the experiment subjects were extensively trained on a behavioral task requiring precise fixation. Eye tracking was performed using an infrared video-based eye-tracker at 500 Hz (Eyelink 1000; SR Research). Calibration was performed throughout each session to maintain a validation accuracy of less than 1° average offset from expected using either a 10-point or 13-point calibration procedure. Trials were canceled online when observer’s eyes moved more than 1° away from the fixation cross for more than 300 ms. After training, canceled trials consisted of fewer than 0.1% of all trials. Due to technical limitations eye tracking was not performed inside the scanner.

Experimental design.
Motion stimuli consisted of two patches of moving dots and a central cross (1 × 1°) on which observers maintained fixation. The dot patches were rectangular regions extending from 3.5 to 12° horizontal and −7 to 7° vertical. Each patch was filled with 21 dots/°2, 50% brighter and 50% darker than the gray background (300 cd/m2). Both patches maintained a constant baseline in between trials of 25% contrast and incoherent motion. During a trial, the patches increased in either or both contrast and coherence. To minimize involuntary eye movements, the coherent dot motion direction was randomized to be horizontally inward or outward from fixation on each trial, such that each patch moved in opposite direction. All dots moved at 6°/s updated on each video frame. Motion strength was adjusted by changing motion coherence; that is, the percentage of dots that moved in a common direction with all other dots moving in random directions. Dots were randomly assigned on each video frame to be moving in the coherent or random directions.

We measured the cortical response to a wide range of brief increments of stimulus contrasts and coherences of variable durations while observers performed an independent and asynchronous task at fixation (Fig. 1). Each scan began with a 30-s baseline period (25% contrast, 0% coherence) to allow visual cortex to adapt. Each trial consisted of a brief increment in either or both the contrast and motion coherence of the dot patches. The dot patches then returned to baseline (25% contrast, 0% coherence) for an intertrial interval of 2 to 11 s (mean 6.5 s) randomly sampled from an exponential distribution. The next trial then began synchronized to the next volume acquisition of the magnet. Stimulus increments were chosen to be +0, +25, +50, or +75% above the baseline 25% contrast and +0, +25, +50, +75, or +100% above the baseline 0% coherence and lasted for 250, 500, 1,000, 2,000, 2,500 or 4,000 ms (or as close to these durations as the display frame refresh would allow). We presented trials in two sets; a “complete cross set” in which all combinations of contrast and coherence changes at 2,500 ms duration were presented (4 contrasts×5 coherences = 20 conditions) and a “duration set” in which a subset of the contrast and coherence combinations (+25 or +75 contrast and +25 or +100 coherence) were presented for variable stimulus durations (4 contrast and coherence combinations×5 stimulus durations = 20 conditions). Thus, across the complete cross and duration sets, there was a total of 40 conditions (20 each in the complete cross and duration sets). For each condition we acquired a minimum of 20 repeated presentations throughout the scan sessions of each observer, resulting in a minimum of 800 trials total. The two trial sets were presented in separate scans interleaved within sessions. Condition order within each scan, for both trial sets, was randomized independently for the stimulus on the left and right such that in every block of 40 trials all conditions were presented in both dot patches.

Fig. 1.
Fig. 1.
Cortical measurement experiment. Observers were shown patches of moving dots that increased in contrast and motion coherence on each trial. A 30-s baseline period preceded each scan with 25% contrast dots and incoherent motion and the baseline dots persisted between trials. On each trial the contrast increased by 0, 25, 50, or 75% and the coherence by 0, 25, 50, 75, or 100% for a stimulus duration of 250 to 4,000 ms. Observers performed an asynchronous task at fixation throughout the experiment.

Download figure Download PowerPoint

While these stimuli were being presented for the passive viewing condition, the observer was required to perform a luminance decrement task on the fixation cross. The fixation cross decremented twice in luminance for 400 ms, separated by an 800-ms interstimulus interval and the observer reported with a button press which decrement interval appeared darker (see Gardner et al. 2008 for details). Decrement amplitude was adjusted according to a staircase procedure to maintain ~82% correct.

MRI acquisition and preprocessing.
Visual area mapping and cortical measurements were obtained using a multiplexed sequence on a 3 Tesla GE Discovery MR750 (GE Medical Systems) with a Nova Medical 32ch head coil. Functional images were obtained using a whole-brain T2*-weighted two-dimensional gradient-echo acquisition (FOV = 220 mm, TR = 500 ms, TE = 30 ms, flip angle = 46°, 7 slices at multiplex 8 = 56 total slices, 2.5 mm isotropic). In addition, two whole-brain high-resolution T1-weighted 3D BRAVO sequences were acquired (FOV = 240 mm, flip angle = 12°, 0.9 mm isotropic) and averaged to form a “canonical” anatomical image which was used for segmentation and surface reconstruction and session-to-session alignment. A T2*-weighted scan with the phase encoding direction reversed was collected in each session and used in combination with the FSL function TOPUP to correct for distortions due to high multiplex factors (Andersson et al. 2003). In each functional session, we also obtained a “session” anatomical image for alignment with the canonical anatomy using a T1-weighted 3D BRAVO sequence (FOV = 240 mm, flip angle = 12°, 1.2×1.2×0.9 mm). Analysis was performed using custom MATLAB software (Gardner et al. 2018a).

Session anatomies were aligned to the canonical anatomy and data were displayed on flattened cortical surfaces for visualization and for defining visual areas. Gray matter and white matter segmentation was performed on the canonical anatomy using FreeSurfer (Dale et al. 1999) and flattened triangulated surfaces used for displaying data. Each session anatomy, was aligned to the canonical anatomy using image-based registration (Nestares and Heeger 2000) so that the location of mapped cortical visual areas could be projected into each session’s space. All data analysis was performed in the native coordinate of the functional scan without transformation.

Cortical visual area mapping was performed using a population receptive field mapping technique (Dumoulin and Wandell 2008). Observers performed the fixation task described above while a moving-bar stimulus moved across the visual field in different directions. The measured responses were used to estimate the voxelwise population receptive field and then the eccentricity and polar angle of each receptive fields was projected onto a flattened representation of the cortical surface where visual areas were identified according to published criteria by hand (Gardner et al. 2008; Wandell et al. 2007). Each moving bar stimulus scan lasted 4 min and the same randomization sequence was repeated and averaged eight times to improve the signal-to-noise ratio. The stimulus was a full contrast 3° width bar spanning the entire display. Inside the bar a full contrast cross-hatch pattern of black and white rectangles moved continuously to minimize adaptation. Each of the 4-min scans began with a 12 s blank followed by eight 24-s cycles in which the bar swept across the entire screen in one of the eight cardinal or oblique directions. Two additional 12 s blanks occurred after the third and sixth bar sweeps to help estimate large population receptive fields. The bar swept across the visual field at 2°/s. The screen was crescent shaped and extended ~25° vertical and ~50° horizontal. Beyond the screen boundaries the image was blacked out to prevent artifacts from reflecting on the scanner bore. We were able to consistently map V1-hV4, V3A/B, V7 (IPS0) and hMT+ (referred to as MT; see Huk et al. 2002; Amano et al. 2009) in all observers. Areas LO1–2, VO1–2, and IPS1–3 were not consistently identified and were therefore excluded from analysis.

Motion correction, linear trend removal, filtering, and averaging across cortical visual areas were performed to obtain a single time course for each cortical area for each observer. T2*-weighted images were motion corrected with a rigid body alignment using standard procedures (Nestares and Heeger 2000). Scans within each session were linearly detrended, high-pass filtered with a cutoff frequency of 0.01 Hz to remove low-frequency drifts, converted to percent signal change by dividing each voxel’s time course by its mean image intensity within each scan, and then concatenated across scans.

Analyses of responses of cortical areas were conducted by averaging the time series of voxels whose trial-triggered response across all conditions accounted for the highest amount of variance within each retinotopically defined visual area. Specifically, we performed an event-related analysis to recover the response evoked by each trial (regardless of condition), using the following equation to model voxel responses:

y=Xβ+ϵ
where y is an n×1 array representing the time-series of BOLD response for n volumes from a single voxel. X is an n×k stimulus convolution matrix in which the first column contains a one for the volume when each trial began and zeros elsewhere. Each subsequent column is shifted downwards by one to form a Toeplitz matrix and k was set to 81 to model responses as occurring from the time of stimulus presentation through 40.5 s later. Each voxel is assumed to have additive Gaussian noise with variance ε. By computing the least-squares estimate of the column vector β, we obtained the finite impulse response evoked by all trials, that is, the average response after a trial accounting for linear response overlap. We computed r2, the amount of variance accounted for by this model (Gardner et al. 2005). We then averaged the time series of the top 25 voxels per cortical area sorted by r2. While we chose this voxel selection criterion to produce high signal-to-noise estimates of each cortical areas response, our conclusions did not depend on its use. Repeating the complete analysis using either all voxels in each cortical area, the top two voxels, or all voxels weighted by their receptive field overlap with the stimulus results in a change in the signal-to-noise in the data but did not qualitatively change the key findings.
To examine how the hemodynamic response for each cortical area changed as a function of stimulus condition (Fig. 2), we computed the finite impulse response for each condition in the passive viewing experiment. That is, we computed the finite impulse response as above, but allowed for a separate response for each of the 20 conditions in the cross set and 20 in the duration set. Our complete stimulus convolution matrix therefore had 3,240 columns (81 volumes by 40 conditions), while each observer’s data consisted of at minimum 13,440 time points and up to 30,000 time points in some observers. Solving for the least squares solution results in hemodynamic response for each of the 40 conditions in the experiment which we call the measured cortical response.

Population response functions: overview.
Using the measured cortical responses we then estimated the population response functions for contrast and coherence in each cortical visual area. Our model framework and measurements are available online, as a tool for experiment design and comparison with existing results (Birman and Gardner 2018). Following previous work examining the relationship between contrast or coherence and BOLD response (Avidan et al. 2002; Boynton et al. 1996, 1999; Gardner et al. 2005; Heeger et al. 2000; Logothetis et al. 2001; Olman et al. 2004; Rees et al. 2000; Tootell et al. 1998) we assumed that there was a smooth functional form (linear, exponential or sigmoidal, see details below) between the contrast and coherence of the stimulus and the magnitude of neural response. For each trial, the magnitude of neural response was computed as the linear sum of the response to contrast and coherence predicted by these smooth functions and a trial onset response that was the same across all conditions (interaction terms between contrast and coherence were tested and compared against simpler models by cross-validated variance explained). The neural magnitude was used to scale the magnitude of a boxcar function of the appropriate duration exponentially scaled (see below) to account for nonlinear effects of duration. The resulting time series was then convolved with a canonical hemodynamic response function estimated from the data. The parameters of the population response functions and magnitude of the trial onset response were then adjusted to best fit the event-related responses in the least squares sense through nonlinear fitting routines (active-set algorithm implemented in lsqnonlin in MATLAB). To avoid overfitting and to compare models with different numbers of parameters, we evaluated models according to the cross-validated r2 by performing a leave-one-condition out cross-validation, using 39 of the 40 stimulus conditions to train the model while predicting on the left out condition. We proceeded with this analysis in two steps: characterizing the canonical hemodynamic response and duration effects, and then fitting the population response functions parameters.

Population response functions: canonical hemodynamic response function and duration effects.
We first fit parameters of the canonical hemodynamic response function and duration effects, ignoring the effect of contrast and coherence. To do so we fit the population response model with arbitrary scaling factors (beta weights) for each of the 40 conditions. This approach allowed us to determine the shape parameters of the hemodynamic response function and temporal nonlinearity without being biased by magnitude differences across conditions.

We characterized the shape of the canonical hemodynamic response function for each observer with a difference of two gamma functions:

rcanonical(t)=Γ1(t)−Γ2(t)
Γ(t)=⎧⎩⎨⎪⎪α[t−t0τ]n−1e−1/ττ(n−1)!|t≥00|t<0
where α is the amplitude, t0 is the time lag such that when t < t0 the function is zero, and n and τ control the shape of the function. The parameter α was set such that the peak response to a 500-ms stimulus was 1. Thus the reported percent signal change in the population response functions are relative to a 500-ms stimulus.
We accounted for nonlinear effects of temporal summation (Boynton et al. 1996) in the BOLD response by allowing responses to be exponentially scaled. Small variations in duration are known to scale in an approximately linear manner (Boynton et al. 1996) whereas across large variation in stimulus durations the response to longer durations is less than expected by a linear system (Boynton et al. 2012). We are agnostic to the source of this effect, which could result from either neural adaptation (Buxton et al. 2004) or due to saturation of the BOLD signal (Friston et al. 1998). We took the response of the 500 ms duration stimulus as the baseline and scaled shorter and longer responses according to the inverse ratio of the durations raised to a fit parameter δ (i.e., a 1,000-ms stimulus has a ratio of 1,000500−δ=2−δ). This final value corresponds to the proportion of a linear response that occurred and the boxcar of appropriate duration was scaled by this value.

Altogether we fit the parameters for two gamma functions in the canonical hemodynamic response function (α, t0, τ), the duration effect δ and 40 beta weights for stimulus conditions to the event-related responses. The canonical hemodynamic response function parameters and the duration parameter were then used in the estimation of the population response functional forms while the beta weights were discarded.

Population response functions: functional forms.
To characterize the population responses of each visual area to changes in contrast and motion coherence we fit functional forms to the underlying neural population response functions. We assumed that these population response functions would be monotonically increasing for both contrast and coherence. For contrast, we parameterized the relationship between contrast and neural response as a sigmoidal function (Naka and Rushton 1966) following previous work (Albrecht and Hamilton 1982):

Rcontrast(scontrast)=αcontrast(scontrast1.9scontrast1.6+σ1.6)
where α is the maximum amplitude of the function and σ controls the shape of the function. We fixed the exponent parameters of the Naka-Rushton to 1.9 and 1.6 based on previous work (Boynton et al. 1999).
To avoid making assumptions about the coherence response function we assumed that the form would either be linear or a saturating nonlinearity motivated by previous work (Rees et al. 2000; Simoncelli and Heeger 1998). The saturating nonlinearity was an exponential function but can interpolate smoothly between a linear and nonlinear function.

Rcoherence(scoherence)=αcoherence(1−escoherenceκ)
In the exponential function the parameter κ controls the shape of the function by setting the point at which the exponential function reaches 63% of its maximum and α controls the amplitude. Large values of κ combined with large values of α make this function approach linear in the range [0 1] in which the stimulus strength scoherence is bounded.

To assess whether and to what extent contrast and motion coherence interact we included an additional parameter in the population response function model. The parameter βinteraction scaled the multiplicative effect of contrast and motion coherence according to the following equation:

Rinteraction(scontrast,scoherence)=βinteractionRcontrast(scontrast)Rcoherence(scoherence)
The full model of neural response was computed as the sum of the contrast and coherence response, the interaction term, and a constant stimulus onset effect Ronset.

Rneural(scontrast,scoherence)=Rcontrast(scontrast)+Rcoherence(scoherence)+Rinteraction(scontrast,scoherence)+Ronset
We evaluated the fit of the full model with and without the additional interaction parameter by comparing the cross-validated variance explained. We also fit an alternative interaction model in which different population response functions were allowed to fit for conditions in which only one feature changed (i.e., the first column and last row of the “grid” in Fig. 2A) compared with conditions in which both features changed (other parts of the grid in Fig. 2A).

We fit the free parameters of the population response functions by constraining the fits on each observer’s cortical measurements (Fig. 4). To do this we computed the neural response Rneural and then scaled this by the boxcar of appropriate duration for each stimulus condition. The boxcar was additionally scaled according to the duration parameter. Finally we convolved this scaled boxcar with the canonical hemodynamic response resulting in a predicted hemodynamic response for each stimulus condition.

To evaluate whether the parameters we fit differed across subjects and across cortical areas we fit a linear model for each parameter. We first performed model comparison to establish whether each parameter was better explained by a model with only an intercept, a per-subject effect, a per-area effect, or a per-subject and per-area effect. For each parameter we fit all four models (using the function fitlme in MATLAB) and retained the most complex model which resulted in a statistically significant improvement in prediction, assessed via partial F-test. For each parameter we then investigated which observers and cortical areas showed statistically significant differences relative to the mean parameter value as reported in Tables 1 and 2.

Computing stimulus sensitivity.
For each cortical area we computed various measures of sensitivity to contrast and motion coherence. In particular, we examined the αcontrast parameter, which controls the maximum response of the Naka-Rushton function. Because in the range we measured the slopes are approximately linear and the Ronset term absorbs the stimulus-independent response, αcontrast tracks the slope of the relationship between contrast and response and therefore is a measure of sensitivity to contrast. The parameters of the exponential form of the coherence function we used are not interpretable in isolation so instead we took the population response functions for coherence and measured their response range by performing a linear fit. We report the slope of that fit as the sensitivity to coherence.

The measurements of sensitivity which we report will be sensitive to the signal-to-noise of our measurements. This could be particularly problematic because signal magnitude and variability may depend on whether there are sinuses or large draining veins in a cortical region which are known to have large signals with high variability. Also, differences in signal-to-noise that are due to proximity to receiver coils or partial voluming effects may bias our measurements of sensitivity, particularly making comparisons across different areas problematic. In addition if variance is proportional to the mean as it is expected to be for single neurons or Poisson-like processes (Softky and Koch 1993), then measures of population sensitivity would need to be scaled appropriately as response magnitude grows. We therefore examined the variability of response in each cortical visual area. First, we fit a canonical hemodynamic response function to all trials as described above. We then fit a general linear model using this canonical hemodynamic response and allowed each trial to have a separate beta weight. That is, we found the scale factor (beta weight) for every single trial which best fit the measured time course in the least squares sense, accounting for linear overlap across trials, for each observer for every cortical area. To avoid response variance associated with different stimulus strengths, we grouped the scale factors by condition (20 contrast and coherence; 20 duration) and computed the standard deviation. This results in 3,520 measurements of standard deviation (11 observers × 8 cortical areas × 40 conditions) each of which was computed from ~25 trials. If the microvasculature, coil proximity, or partial voluming in different cortical areas resulted in differences in variability, or if contrast or coherence caused the variability to increase, we would expect that these measurements of standard deviation would consistently vary with those parameters. We tested for this by fitting a series of linear models in which the standard deviation depended on either an intercept alone, each condition’s contrast, coherence, cortical area, or random effect of subject, and all the effects together. We also tested models in which the contrast and coherence effects could differ by area. We performed model comparison by testing for improvement over the intercept-only model via partial F-test.
\section{Results}
Measuring cortical responses to contrast and motion coherence.
We characterized human cortical responses to changes in contrast and motion coherence of patches of dynamic random-dot stimuli by measuring BOLD responses while observers passively viewed two patches of moving dots (Fig. 1). Each scan began with 30 s of baseline stimulus presentation (0% coherence, 25% contrast) after which trials consisting of brief increments (0.25–4 s) in either or both coherence and contrast before returning back to baseline for a random length intertrial interval (2–11 s) (see methods for full details). In total observers were shown 40 conditions: 20 consisted of combinations of changes in contrast (+0, +25, +50, and +75%) and changes in motion coherence (+0, +25, +50, +75, and +100%) for 2,500 ms each, the remaining 20 were a subset of these combinations combined with variable stimulus durations (250, 500, 1,000, 2,000, and 4,000 ms). To minimize task-dependent effects and maintain a consistent level of engagement, observers performed an independent fixation task during viewing. We computed hemodynamic responses to each stimulus condition for each observer using an event-related analysis for retinotopically defined visual areas V1, V2, V3, hV4, V3A, V3B, V7, and MT. We begin by describing responses in visual areas V1 (Fig. 2A) and MT (Fig. 2B), as they are well known to be sensitive to contrast (Avidan et al. 2002; Boynton et al. 1996, 1999; Gardner et al. 2005; Logothetis et al. 2001; Olman et al. 2004; Tootell et al. 1995, 1998) and motion coherence (Britten et al. 1993; Händel et al. 2007; Rees et al. 2000; Simoncelli and Heeger 1998), respectively.

Fig. 2.
Fig. 2.
Measurements of event-related responses in cortical areas V1 and MT. A: cortical area V1. To obtain the individual responses shown here we performed an event-related analysis on our time series. In total we included 40 conditions in the experiment: 20 consisted of a full cross of changes in contrast and/or coherence presented for 2,500 ms (shown in bold in the grid in the bottom left) and 20 were a subset of the full cross conditions presented for various durations (shown in diagonal for the four conditions with additional durations recorded). We measured cortical responses to changes in contrast (top left) where each trace is averaged over changes in coherence, i.e., each response is the average of a row in the bottom left grid. We also measured responses to changes in coherence (bottom right), each trace is averaged over changes in contrast, i.e., each response is the average of a column in the grid. We made additional measurements across a large range of stimulus durations (top right) also shown in the grid. B: as in A for cortical area MT (hMT+). In all panels the event-related responses are averaged across observers and error bars indicate the bootstrapped 95% confidence interval; some error bars may be hidden. Note that for visualization event-related responses are only shown out to 15 s but the analysis used a window of 40.5 s.

Download figure Download PowerPoint

We observed clear parametric sensitivity to increases in contrast in V1 but weaker sensitivity in cortical area MT. Our measurements in V1 confirm previous results (Gardner et al. 2005; Logothetis et al. 2001; Tootell et al. 1995, 1998). The contrast sensitivity of V1 can be appreciated as monotonically increasing response magnitudes for higher levels of contrast increments (top left orange traces, Fig. 2A). These traces are for a stimulus duration of 2.5 s collapsing across motion coherence increments, i.e., averaging each row in the full response grid. While MT was also sensitive to increments of contrast, the monotonic increase appeared less pronounced compared with V1 (top left orange traces, Fig. 2B), consistent with other reports that have noted MT as having near maximal responses to small changes to contrast (Sclar et al. 1990; Tootell et al. 1995).

For motion coherence, we found the opposite pattern: MT was much more sensitive to increments in motion coherence compared with V1. MT showed clear monotonic increasing responses with increasing motion coherence (bottom right purple traces, Fig. 2B). These traces are again for a stimulus duration of 2.5 s averaged over contrast increments, i.e., collapsing each column in the full response grids. In V1 there was little difference in response amplitude as a function of motion coherence, i.e., weak sensitivity to coherence (bottom right purple traces Fig. 2A).

While V1 showed little parametric sensitivity to difference in coherence and MT little sensitivity to difference in contrast, both show a large response to the smallest increment of these parameters. This consistent trial-by-trial response, which we call the stimulus-onset response, appears unrelated to our parametric manipulations. For example, despite showing little sensitivity to different levels of coherence all of the responses for V1, including the one induced by the least change in coherence (+25%), induced a large response relative to the baseline (purple traces, Fig. 2A). Similarly, for MT and contrast as can be appreciated by noting that increasing contrast by 25% (orange traces, Fig. 2B) resulted in a large response. Part of this apparently large response is due to the fact that these responses for contrast or coherence are averaged over changes in the other parameter. That is, increases in contrast are shown averaged over coherence and vice versa. However this is not the complete story as can be appreciated by examining the grid of responses to each parameter separately (small bold black traces in grid, Fig. 2A and B). V1 can be seen to respond to a small change in coherence (+25, along horizontal) when there is no change (+0, along vertical) in contrast and vice versa for MT. These relatively large responses, to a feature each area is not strongly sensitive to, suggests that there is a response to stimulus onset regardless of condition.

Motion visibility is also adjusted by reducing the duration of stimuli, often in conjunction with reduced contrast and coherence. Along with the measurements described above, for which the stimulus duration was 2.5 s, we tested a large array of different durations from 0.25 to 4 s. As expected of an approximately linear system (Boynton et al. 2012) we observed that responses scaled with stimulus duration in both cortical areas V1 and MT (top right gray traces, Fig. 2A,B).

Across the rest of the visual areas that we were able to retinotopically define in all subjects (V2, V3, hV4, V3A, V3B, and V7) we found similar parametric sensitivity to contrast, motion coherence and stimulus duration (Fig. 3). In general, and in concordance with previous reports (Avidan et al. 2002) we found less parametric sensitivity to changes in contrast for visual areas higher up in the visual hierarchy in the range we measured (+25 to +75% contrast). Sensitivity to coherence was observed in a number of the visual areas, although MT and to a lesser extent V3A were the clear stand-outs in showing monotonically increasing responses to this parameter. These observations will be quantified below.

Fig. 3.
Fig. 3.
Measurements of event-related responses in cortical areas V2–V7. A–F: conventions are the same as in Fig. 2.

Download figure Download PowerPoint

Fitting population response functions to cortical responses.
To quantify the parametric sensitivity to contrast and coherence of each visual area we fit the event-related responses with a population response model using idealized functional forms for the relationship between contrast and coherence and neural response (Fig. 4). Based on previous work we expected that the population response to contrast would be a sigmoidal function (Albrecht and Hamilton 1982; Sclar et al. 1990; Boynton et al. 1999) with the form of a Naka-Rushton equation (Fig. 4B, orange curve) (Naka and Rushton 1966). To avoid overfitting, we fixed the exponents in the equation based on previous work (Boynton et al. 1999) and only allowed σ and αcontrast to vary. For motion coherence, we allowed for a functional form that can smoothly interpolate between linear (Britten et al. 1992, 1993; Simoncelli and Heeger 1998; Rees et al. 2000) and a saturating exponential (Fig. 4B, purple curve). Finally, we included an onset term to capture the portion of response that did not vary across all conditions which presumably reflects stimulus onset and not parametric variation of stimulus parameters.

Fig. 4.
Fig. 4.
Population response function model. A: Each condition in the experiment was defined by three parameters: the increment in contrast above baseline (+0, +25, +50, or +75%), the increment in coherence above baseline (+0, +25, +50, +75, +100%), and the stimulus duration (250, 500, 1,000, 2,000, or 4,000 ms). As an example we use condition 1 to demonstrate the model. B: to estimate the response to each feature within a condition we first find the change in response due to the corresponding change in stimulus intensity according to the population response functions. For contrast the population response function is a Naka-Rushton with two free parameters: αcontrast controlling the amplitude and σ the shape. For coherence the response function was a saturating nonlinearity with two free parameters: αcoherence controlling the amplitude and κ the shape. We added the resulting change in response together (while testing for interaction effects, see methods) and included an onset parameter to account for stimulus response that did not vary parametrically with the stimulus features. C: the total response, including onset, was used to scale a boxcar function whose length matched the stimulus duration. The boxcar was additionally scaled by a parameter to account for the nonlinear effect of stimulus duration. D: the resulting boxcar was convolved with a canonical hemodynamic response function fit separately for each observer. E: the model outputs a prediction for each condition about the expected event-related response (red lines). The parameters within the population response function model were then optimized to minimize the sum of squared errors between the data (black markers) and the model responses.

Download figure Download PowerPoint

To predict the BOLD response from the modeled contrast and coherence response functions, we employed a linear-systems approach (Heeger et al. 2000; Rees et al. 2000; Logothetis et al. 2001). To account for different durations of stimuli, we multiplied the response magnitude predicted by the onset, contrast, and coherence functions with a boxcar function of appropriate length (Fig. 4C). As it is known that brief stimuli evoke response larger than expected by linearity (Boynton et al. 1996, 2012), we also scaled the boxcar magnitude with an exponential that accounted for this nonlinearity in response. This scaled boxcar was then convolved with a hemodynamic response function (Fig. 4D) whose parameters were adjusted to best fit the event-related responses across all conditions (Fig. 4E). All together, we fit the model parameters for the contrast function (αcontrast, σ), coherence function (αcoherence, κ) and temporal effects (δ, Ronset), and the parameters for the hemodynamic response function (t0, τ0, t1, τ1, α1) for each observer for each visual area by minimizing the sum of least squares between the output of the model and the event-related responses for each of the 40 conditions.

We report the main fit parameters of the hemodynamic response function and population response function model across cortical areas (Table 1) and observers (Table 2). We assessed whether between-observer variability existed by fitting a linear model predicting each parameter with observers as categorical predictors and used the same procedure to assess for within-observer variability across cortical areas (see methods). We found that there was statistically significant between-observer variability across all of the parameters but only significant variability within-observer (i.e., across cortical areas) for the shape parameter of the hemodynamic response τ, the magnitude and shape parameters of the contrast response function αcontrast and σ, the parameters of the coherence response function αcoherence and κ, and the onset parameter Ronset (significance established by a partial F-test comparing linear regression models with and without each group of additional parameters at the P = 0.05 threshold). Note that the κ and αcoherence parameters which together control both the shape and magnitude of the coherence response are hard to interpret in isolation.

Table 1. Variability in parameter estimates across cortical areas

Enlarge table
Table 2. Variability in parameter estimates across observers

Enlarge table
The population response model was able to capture the majority of variance in each observer’s event-related responses and a significant portion of this explained variance was accounted for by the population response functions. We assessed variance explained as the squared correlation between the model predictions and the actual event-related responses for held-out conditions. For V1, r2 = 0.69, 95% CI [0.63 0.75]; V2, r2 = 0.63, 95% CI [0.58 0.68]; V3, r2 = 0.62, 95% CI [0.56 0.68]; hV4, r2 = 0.44, 95% CI [0.35 0.53]; V3A, r2 = 0.42, 95% CI [0.35 0.50]; V3B, r2 = 0.38, 95% CI [0.31 0.46]; V7, r2 = 0.32, 95% CI [0.24 0.40]; MT, r2 = 0.49, 95% CI [0.43 0.56]. Part of the variance accounted for by the model is simply due to the stimulus-onset term and hemodynamic response, but the population response functions also captured significant variance. We assessed this by comparing our results to a model fit to the same measurements but where the condition labels were permuted. This corresponds to keeping the variance explained by stimulus onset and the hemodynamic response but randomizes the relationship between condition and response. We repeated this permutation test procedure 100 times per observer and cortical area. On average across observers and areas the variance explained by fitting to the measured data set (average cross-validated r2 = 0.508) exceeded the variance explained in the permuted data set (average cross-validated r2 = 0.340) with P < 0.001, Δr2 = 0.164, 95% CI [0.162 0.165].

Across cortical visual areas the model captured the response to changes in contrast and motion coherence as well as the amplitude effects due to duration. To visualize the fit of the population model to each variable we scaled the canonical hemodynamic response function for each observer to fit the event-related responses in the conditions with either no change in contrast or no change in coherence. This results in a single scaling factor for each of these conditions (circles, Fig. 5) which we compared with the model predictions (lines, Fig. 5). Examination of the magnitude of the population model fit to the event-related response peaks for changes in contrast (orange curves, Fig. 5A) and coherence (blue curves) shows good correspondence. This is particularly notable given that the model is fit across all conditions containing different response lengths, as well as combinations of contrast and coherence changes, while the displayed data are for changes in contrast and coherence in isolation. This visualization displays a model fit to all the data, i.e., not on held-out data, but with similar explained variance to the cross-validated model (difference between cross-validated and full fit, Δr2 = 0.005, 95% CI [0.004 0.006]). The population response functions echoed the qualitative results described above for the event-related responses: V1-hV4 showed strong response to contrast with relatively weak response to coherence. Only MT showed stronger response to motion coherence than to contrast. Moreover, the amplitude of responses as a function of duration (Fig. 5B) were similarly well captured by the population response model. As noted earlier the amplitude of responses due to doubling in duration do not appear to scale in a linear manner.

Fig. 5.
Fig. 5.
Population response functions. A: the population response functions fit to each cortical area V1-MT (hMT+) are shown compared with the magnitude of the event-related response for the conditions in which only one feature changed. These correspond to the conditions in the first column and last row of each event-related response grid in Figs. 2 and 3. To make the functions comparable to the data in an easy to interpret space we reduced each event-related response to a single magnitude value which was obtained by finding the linear scaling of the canonical hemodynamic response to that condition. The model outputs predictions for all 40 conditions but we are only showing the subset where either contrast or coherence changed alone. Note that the predictions here are not out of sample (i.e., these are not the cross-validation results) but we show the full fit to better visualize the response functions. B: as in A but for the variable duration conditions in which contrast and coherence changed maximally (+75% contrast, +100% coherence). In all plots markers indicate the average across observers and error bars the bootstrapped 95% confidence interval.

Download figure Download PowerPoint

The form of the contrast response function has been extensively studied (Albrecht and Hamilton 1982; Boynton et al. 1999; Sclar et al. 1990) while the motion coherence response function has received much less attention. Single-unit studies have found a linear response function, whereas BOLD measurements in humans have found some nonlinearity of response, particularly outside of MT (Rees et al. 2000). We therefore tested for nonlinearity in the population response functions to motion coherence and found that responses were generally best characterized as linear, with a small deviation from linearity for MT. We quantified this comparison as the difference in cross-validated variance explained between the saturating exponential and a linear form for the coherence response function. In MT we found a small difference in favor of the nonlinear model Δr2 = 0.004 (95% CI [0.001 0.007]) while all other cortical areas’ confidence intervals overlapped with zero. This difference is visible as the saturation of the MT coherence response to large changes in coherence (Fig. 2B and Fig. 5A, MT).

While population responses to each motion feature could interact, i.e., a change in contrast might influence the response to a change in coherence or vice versa, we found no evidence for this. We tested for interactions by adding an additional beta weight to the model accounting for the effect of multiplicative changes in contrast and coherence (see methods section Population responses: functional forms). Including this term reduced the cross-validated variance explained by on average −6.67% (95% CI [−13.42, 0.08]) across cortical areas, suggesting overfitting compared with the no-interaction model. One observer’s data was particularly strongly overfit. Removing that observer resulted in an average reduction in variance explained of −0.08% (95% CI [−0.25 0.09]) and for individual areas, V1: −0.18%, V2: −0.16%, V3: −0.17%, hV4: −0.13%, V3A: −0.07%, V3B: −0.14%, V7: −0.07%, MT: −0.11%.

Visual inspection of the response grids (bottom left panels in Fig. 2 and 3) suggest an alternative kind of interaction in which the response to contrast and coherence might be stronger in the absence of the other feature changing. Take for example the response to contrast compared with coherence in V1. The contrast response in V1 is so much larger than the response to coherence that it’s possible it “washes out” any visible effect due to coherence. To test for this possibility we fit a model with different population response functions for conditions in which only a single feature changed vs. when both features changed. We found that these models were also not statistically better than the simplest model with no interactions: average reduction in cross-validated variance explained −5.34% 95% CI [−9.13, −1.56] and without the overfit observer −0.20% 95% CI [−0.31, −0.08]. Although statistically the models were similar in our data set we did find that in the interaction model the population response functions to contrast had a higher maximal response when the coherence was not simultaneously changed, but the reverse was not true. On average across subjects and cortical areas we found an increase in sensitivity of ~50% in the contrast response when no simultaneous change in coherence occurred (average parameter change 1.68 95% CI [0.58, 2.78], significantly different from zero as assessed by bootstrap over observers, P = 0.007).

The population response model fits (Fig. 5) replicate earlier reports showing that contrast responses have a smaller dynamic range and saturate more quickly in higher visual cortical areas (Avidan et al. 2002), and add the finding that coherence sensitivity peaks in MT. To assess this we plotted the maximum of the contrast response function (the αcontrast parameter) against the linear slope of the coherence response function (the response range measured as the slope of a linear fit, see methods) for each cortical area (Fig. 6A). As expected we found stronger sensitivity to motion coherence in V3A and MT compared with area V1 (Dupont et al. 1994; Tootell et al. 1995; Watson et al. 1993; Zeki et al. 1991). The difference in coherence sensitivity between V3A and V1 was 0.167, P < 0.001 and between MT and V1 0.251, P < 0.001. But we also observed significant sensitivity to changes in coherence in all regions measured (Fig. 6B): V1 = 0.12, V2 = 0.19, V3 = 0.18, hV4 = 0.13, V3A = 0.25, V3B = 0.15, V7 = 0.22, MT = 0.36, slopes in % signal change/unit coherence, all P < 0.001 assessed by bootstrap across observers. All cortical visual areas showed statistically significant parametric sensitivity to changes in contrast (Fig. 6C) assessed as a nonzero αcontrast parameter by bootstrap across observers, all P < 0.001 except MT, P = 0.002. The maximum contrast response dropped quickly for regions higher in the visual hierarchy (V1 = 2.00, V2 = 0.87, V3 = 0.68, hV4 = 0.63, V3A = 0.35, V3B = 0.24, V7 = 0.33, MT = 0.20, units in % signal change/unit contrast).

Fig. 6.
Fig. 6.
Cortical sensitivity to contrast and motion coherence. A: to obtain a qualitative estimate of cortical sensitivity to each motion visibility feature across the cortical visual areas we plotted the αcontrast parameter from the Naka-Rushton function against the slope of a linear fit of the coherence functions. B: the slope of the coherence functions fits as in A replotted with individual subjects. C: the αcontrast parameter as in A replotted with individual subjects shown for each cortical area. D: we plot the ratio of the sensitivity parameters as an unbiased additional comparison because the amplitude parameters could be sensitive to the signal-to-noise ratio of the measurement in different cortical areas. Note that for some subjects the slope of the coherence response was near zero in some cortical areas, we note these as a ratio of infinity (Inf). The means are calculated excluding infinite values. E: the stimulus-onset response parameter Ronset indexes the portion of the response that was not parametrically modulated by contrast or coherence. F: for each doubling in stimulus duration the proportion of response increase is shown by cortical area where 100% would indicate that responses increased linearly with duration. In all panels markers indicate the mean and error bars the bootstrapped 95% confidence interval. Error bars are omitted in panels (B–E) for visualization.

Download figure Download PowerPoint

Although we fit a Naka-Rushton function to the contrast response our measurements were limited to only a few points (no change in contrast, +25, +50, and +75%). This meant that the data did not strongly constrain a sigmoidal fit. We assessed whether in our data set the results would be equally well fit by a linear model and found that this was the case for all areas except V7, with an average improvement of 0.32% in cross-validated variance explained. Therefore, the αcontrast parameter which fits the maximal response to contrast in each region tracks the slope of the relationship between contrast and response and can therefore be used as a measure of the sensitivity to contrast, in the range of contrasts we measured. The linear model’s improvement in variance explained for individual areas were V1 0.66, 95% CI [0.16 1.15]; V2 0.79, 95% CI [0.14 1.45]; V3 0.51, 95% CI [0.04 0.98]; V4 0.27, 95% CI [0.09 0.45]; V3a 0.24, 95% CI [0.06 0.42]; V3b 0.17, 95% CI [0.03 0.31]; V7 −0.04, 95% CI [−0.14 0.06]; MT 0.21, 95% CI [0.07 0.34].

We found that the variability in our measurements did not differ significantly across different cortical areas or according to the stimulus strength. We performed this analysis to test whether various nuisance variables could have altered our measurements, e.g., proximity to the coils and partial voluming might affect signal-to-noise in different cortical areas, or the variability in our measurements might increase with response magnitude as contrast and coherence cause populations of neurons to be more active. To do this we estimated the response magnitude of every trial and grouped these by condition and cortical area, then fit a series of linear models to see whether variability differed. We found that none of the additional variables improved the model fit over the intercept-only model at the P = 0.05 significance threshold. Importantly, the model that allowed separate values for each cortical area did not improve the model fit, suggesting that response variability did not significantly differ between cortical areas (mean cortical area standard deviation was 1.50 percent signal change; V1 1.75, 95% CI [1.33 2.17]; V2 1.08, 95% CI [0.91 1.25]; V3 1.28, 95% CI [1.06 1.49]; V4 1.37, 95% CI [1.03 1.72]; V3a 5.71, 95% CI [1.28 10.15]; V3b 1.14, 95% CI [0.96 1.31]; V7 1.32, 95% CI [0.97 1.66]; MT 1.30, 95% CI [0.95 1.66]). In addition we found that there was no statistically significant change in variability in the slope of the relationship between variability and stimulus strength (even when separate slopes were allowed for different cortical areas), suggesting that noise in our measurements was additive, i.e., did not increase with increasing response magnitude. Fitting the model with a slope for contrast and coherence (shared across areas) results in a slope of −1.23 percent signal change per unit contrast, t(2,557) = −1.27, P = 0.21, and a slope of −0.58 percent signal change per unit coherence, t(2,557) = −0.78, P = 0.44.

Although our measurements do not suggest that any bias is introduced by potential signal-to-noise differences across areas, we computed the ratio of the contrast and coherence slope parameters as an additional unbiased analysis (Fig. 6D). This ratio allows for between region comparison of the sensitivity to contrast and coherence because the ratio reports how sensitive each region is to contrast compared with coherence and not overall sensitivity. That is, the ratio should be invariant to differences in signal-to-noise, under the assumption that contrast and coherence sensitivity are equally affected. In line with our previous results we found that V1 has a ratio of contrast to coherence sensitivity that is at least an order of magnitude more than the other areas. In addition MT was found to have a ratio near 1 and lower than the other cortical areas, reflecting its stronger relative sensitivity to coherence.

We found that the portion of the BOLD response that did not vary parametrically with contrast or coherence, the stimulus-onset response Ronset did vary across cortical areas (Fig. 6E and Table 1). On average the onset response was 0.23 percent signal change across observers and cortical areas. The stimulus-onset response in V1 and V3 were larger than average at 0.42 percent signal change, 95% CI [0.28, 0.55], while areas V3A, V3B, and V7 were smaller than average, 0.14, 95% CI [0.07, 0.20]; 0.15, 95% CI 0.11, 0.18]; 0.09, 95% CI [0.06, 0.13], respectively. The other cortical areas’ onset effects were V2 0.28, 95% CI [0.19, 0.37]; V3 0.22, 95% CI [0.22 0.36]; V4 0.23, 95% CI [0.15, 0.31]; MT 0.24, 95% CI [0.19, 0.29].

Finally, we found that the effect of increasing stimulus duration was not consistent across cortical areas (Fig. 6F). We found that early visual cortex, V1 in particular, was significantly more sensitive to changes in duration than later visual areas, especially MT. The effect of a doubling in duration on the population response, as a proportion of that expected from a linear model, was 68.56%. On average across subjects we found that V1 and MT differed significantly from the average. We found that the effect of a doubling in duration in V1 was 83% of the linear model, 95% CI [76.00, 92.27], suggesting that V1 is more sensitive to stimulus duration. By contrast in MT the effect was only 62% of the linear model, 95% CI [57.06, 66.08], suggesting that MT may have a more transient response. The effects in other areas were not significantly different from the average: V2 73%, 95% CI [67.20, 79.70]; V3 70%, 95% CI [65.05, 74.95]; V4 70%, 95% CI [64.21, 76.59]; V3A 67%, 95% CI [61.24, 73.31]; V3B 64%, 95% CI [60.45, 67.64]; V7 64%, 95% CI [58.07, 70.02].
\section{Discussion}
We have developed a quantitative framework for modeling human cortical response to motion visibility as parameterized by image contrast, motion coherence, and duration. Our results provide a comprehensive view of the variability in cortical sensitivity to these features, each of which is a critical component of visual stimuli often manipulated in experiments designed to understand visual perception and decision-making. Our measurements show that the range of responses to different levels of contrast was larger in early visual cortex, especially V1, and the range of responses for coherence larger in V3A and MT (hMT+). Nonetheless, a change in either feature caused a cortical response in all the retinotopic areas we mapped. Our results weigh on various other findings in the literature: the precise shape of population response functions, the influence of stimulus duration on cortical signals, and whether or not sensory representations for different features interact. Finally, we believe that this parameterized model, and parametric models in general, suggest mechanisms for the read out of sensory representations from population responses and have therefore made our measurements and framework available online as a resource (see methods).

We studied changes in contrast, coherence, and duration to measure human cortical response within a range where typical human perceptual experiments are performed. One choice we made was to measure contrast from a relatively high baseline. Because the contrast response function is known to adapt to the current background stimulus without altering the form of parametric modulation (Ohzawa et al. 1982, 1985; Sclar et al. 1985, 1989; Gardner et al. 2005) the relative sensitivities we measured should hold at other baselines. With this design we were also able to show that sensitivity to changes in contrast and coherence do not interact. The interaction analysis would be impossible in stimuli where the dots appear from a black or gray background such that both contrast and coherence always change together (Britten et al. 1993; Rees et al. 2000). When designing the dot motion stimulus we also had to ensure that there were sufficient dots and a large enough aperture to be clearly visible and generate a reliable coherence response. At low dot densities the response to changes in coherence are negligible (Smith et al. 2006) and small aperture sizes can cause changes in coherence to result in decrements in response (Ajina et al. 2015; Becker et al. 2008; Costagli et al. 2014). By creating a large stimulus with high density we guaranteed that our dot motion would blanket the population receptive fields of all the cortical areas measured.

We set our stimulus to move at a constant rate of 6°/s, within the peak range of speed tuning in visual cortex, and used a dot stimulus rather than gratings to avoid having spatial frequency tuning affect our measurements. Although individual V1 and MT neurons in the macaque differ greatly in their speed tuning the average tuning of the population is quite similar and centered near 6°/s with ranges that extend far above and below that (Priebe et al. 2006). Measurements of speed tuning in humans evidence broad variability across all of visual cortex but our chosen speed is within the peak range (Singh et al. 2000; Hammett et al. 2013). One common concern with speed tuning in gratings is that spatial frequency tuning differs across cortex and directly impacts sensitivity to other stimulus properties, such as image contrast (Priebe et al. 2003, 2006). We used a random dot stimulus with a wide range of spatial frequency components rather than gratings with a specific spatial frequency to avoid this confound. In principle our stimulus drives neurons with a wide range of tunings and by averaging over voxels in each cortical area we reduce the impact of columnar and other local microstructure in each area (Liu and Newsome 2003; Sun et al. 2007).

We reported here several parameters which together defined the population response functions, but which of these represents a good measure of the sensitivity of a region? We use the term sensitivity to capture parametric differences in response magnitude with differences in contrast or coherence. Thus, an area with high contrast or coherence sensitivity is one in which the response to the lowest and highest values of these parameters evoke the largest difference in response (see methods for how the reported parameters correspond to this). This measure can be used to compare with human behavioral contrast or coherence discrimination performance since signal detection theory predicts that perceptual sensitivity, d′, is directly proportional to this difference (Boynton et al. 1999; Newsome et al. 1989; Pestilli et al. 2011; Tolhurst et al. 1983; Zenger-Landolt and Heeger 2003). However, d′ is also inversely proportional to the standard deviation of response which could vary across different areas, particularly for measurement related reasons that would therefore distort our measures of sensitivity. Our analysis of the variability of response across different areas did not find differences, thus suggesting that our measures are an accurate reflection of contrast and coherence sensitivity. Moreover, we used a selection criterion to analyze a subset of voxels that show consistent trial-to-trial responses to reduce the effect of measurement noise but our parametrization will still be sensitive to any noise that remains.

Response variability might also change with response amplitude as it is known to do for single-unit responses. Although occasionally single neurons can be found that match perception (Britten et al. 1992), groups of neurons (Tolhurst et al. 1983) or larger populations (Averbeck et al. 2006; Zohary et al. 1994), depending on the correlation structure in the population, are likely to more closely reflect perceptual reports. Supporting the idea that populations are used for perceptual readout is evidence from human work where at the coarse resolution of the BOLD signal, which pools over large numbers of neurons, cortical measurements closely track perception under an assumption of additive noise (Boynton et al. 1999; Hara and Gardner 2014; Pestilli et al. 2011; Sapir et al. 2005). In line with this the variance of population responses measured with voltage-sensitive dyes do not change with magnitude of response in V1, i.e., they are additive (Chen et al. 2006). Our own measurements support the hypothesis that populations are subject to additive noise: we found that as contrast and coherence increased and caused larger magnitudes of response we found no evidence that trial-by-trial variability changed. Together our data and previous results suggest that measures of the slope in the BOLD signal population response function are indeed measures of sensitivity and leaves us with a testable prediction: if parameters measure sensitivity (i.e., signal-to-noise ratio) then they should be relatable to human perception under additive noise but not noise which scales with response magnitude.

We observed a saturation of the cortical response to motion coherence that differs from recordings of a linear response in MT in human (Händel et al. 2007; Rees et al. 2000) and monkey (Britten et al. 1993). Saturation of the contrast response function is thought to be the result of normalization, a canonical computation in cortex (Baker and Wade 2017; Carandini and Heeger 2012). If the response to motion coherence is linear, it might suggest that similar normalization does not apply. In fact, models of the V1 to MT circuitry include explicit normalization (Simoncelli and Heeger 1998) and the normalization strength alters whether the model predicts linear or saturating responses. This may account for the discrepancies of results; i.e., normalization may result in weak saturation of coherence response as we have found, in line with evidence from both humans (Costagli et al. 2014; Rees et al. 2000) and monkeys (Britten et al. 1993). In support of this idea is evidence that in the absence of a normal input from V1 the coherence response function in MT becomes more linear, possibly reflecting an increased input from subcortical regions whose coherence response is linear (Ajina et al. 2015). To clarify this we can again turn to behavior. Because the MT response has been linked to behavior (Katz et al. 2016) our model makes a testable prediction: under the assumption that the visual system performs signal detection subject to additive noise (Boynton et al. 1999) a saturating coherence function would predict worse discriminability of coherence at higher base levels of coherence.

To build out our quantitative framework we measured responses to stimuli of varying durations, down to those typically used in psychophysical experiments (e.g., 0.25 s) as well as at durations more typically used for BOLD measurement (e.g., 4 s). Our results confirm many previous results showing that there exists a nonlinearity in the BOLD response, such that shorter stimuli have a larger response than expected by temporal linearity (Boynton et al. 1996, 2012). Modeling our responses, we found that on average across cortical areas a doubling of the stimulus duration was associated with an increase in response of only 67% of the expectation of a linear model. Whether or not this is due to neural adaptation (Buxton et al. 2004) or saturation of the BOLD signal (Friston et al. 1998) cannot be determined from our data. We also observed a slight difference in the duration effect across cortical areas. In V1 increasing duration results in a larger effect on the population response whereas MT showed a smaller than average response, which could be a result of the more transient response in MT (Stigliani et al. 2017).

We also noted that any change in the stimulus, regardless of the type, amplitude, or duration resulted in what we refer to as a stimulus-onset response in all cortical areas. What is the nature of this response? Early recordings comparing BOLD responses to electrophysiological recording suggest that the BOLD signal may be thresholded at some minimum response even though neural activity continues to be modulated below that threshold (Logothetis et al. 2001). Another possibility is that the stimulus-onset response may be the result of a consistent trial structure causing anticipatory responses (Cardoso et al. 2012). In the latter case fitting a separate stimulus onset parameter to absorb this trial-structure related variance is appropriate to correctly estimate the population response from the BOLD signal.

Our approach to making a parametric model of cortical response to motion visibility contrasts with more complex models, such as Gabor wavelet pyramids and deep convolutional networks (Kay et al. 2008; Kay and Yeatman 2017; Yamins et al. 2014) that are typically image-computable and thus can make detailed predictions of cortical response properties directly from images. A complete image computable model would implicitly contain our parametric model within it and seemingly obviate the need to parameterize stimulus visibility and its relationship to cortical response. Building such complex models is a worthy goal, however, we would note that much success in understanding visual cortex function has come from experiments which parametrically altered visual features, in particular features related to visibility. Consider the result of stimulus combination. When two gratings with different luminance contrast are presented the evoked response is not well captured by simple rules such as linear summation or winner-take-all (Busse et al. 2009) Instead, across a large range of parameter combinations the evoked response is well explained by normalization (Carandini and Heeger 2012). The canonical rule that an evoked response should be scaled by the response of a neighboring region of cortex is easily understood in a parametric model, but far less intuitive in a complex one. Another low-dimensional parametric model is the population receptive field (Dumoulin and Wandell 2008; Wandell and Winawer 2015), which has been widely used to map and interpret the properties of retinotopic visual cortex, largely because of its simplicity. In general, low-dimensional quantitative frameworks like the one we have built can parameterize cortical response to key stimulus properties and by doing so, serve to make testable predictions for perceptual function. For example, our framework suggests that small variation in sensitivity across cortical areas might be used to separately determine the visibility of motion for different parameters. That is, a read-out of the visual representations could take advantage of the differences in feature sensitivity by differentially weighting V1 and MT for contrast discrimination and vice versa for coherence discrimination.

Each parameter of motion visibility that we studied has been separately used to uncover the neural basis of different aspects of perception and perceptual decision making. The quantitative framework that we have proposed here shows that despite their similar effects on perception, contrast, coherence, and duration have distinct cortical representation at the level of populations. In studies of perception, the effects of these parameters on cortical response should not be considered to be interchangeable. With our reference framework one can now make changes in one parameter or the other and predict how this will affect human cortical response. In this way our predictive model is a key tool in furthering the goal of linking cortical response to perceptual behavior.

\chapter{Aim 2: Evaluating fixed and flexible readout as mechanisms of selective attention}
\section{Introduction}
\section{Methods}
\section{Results}
\section{Discussion}

\chapter{Aim 3: Comparing spatial and feature-based attention in behavior and mechanism}
\section{Introduction}
\section{Methods}
\section{Results}
\section{Discussion}

\chapter{Discussion and future directions}
\section{Summary and significance of findings}
\section{Directions for future investigations}
...
\appendix
...
\bibliographystyle{plain}
\bibliography{library}
\end{document}