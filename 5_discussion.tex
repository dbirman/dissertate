

Together, these results demonstrate that human selective visual attention is made up of multiple computational components which are shared across different forms of sensory selection. These findings were possible through the use of \textit{computational linking models} which make hypotheses about the connections between neural representation and perception explicit and testable \citep{Barlow1972-kz,Brindley1960-gq,Cohen2010-xs,Newsome1989-fr,Pestilli2011-gi,Hara2014-mv,Gardner2015-bd}.

In Aim 1 I sought to quantify the extent to which cortical changes during selective visual attention could account for perceptual changes. We chose to study the visibility of motion for this purpose. Motion visibility can be controlled by several different perceptual parameters: contrast, coherence, and duration. This makes motion visibility an excellent tool to investigate how cortical changes might be connected to the perceptual enhancements during attention. Prior to this project nobody had laid out a full framework for how motion visibility is represented in human visual cortex. The first step was therefore to build a framework for this purpose. 

I then measured how the sensory representation of motion visibility changed during directed attention and demonstrated that these changes were insufficient to account for perception. We validated that a linking model of motion visibility perception could be built, extending an existing linking model of contrast discrimination \citep{Boynton1999-jd}. We then measured how sensory representations changed and, passing these through the linking model, showed that the scale of changes were too small to account for perception. Based on these observations we suggested that a \textit{flexible readout} must change how signals are gated from sensory cortex into decision-related regions.

The findings in Aim 1 suggested that a substantial amount of the processing during sensory selection occurs outside of the areas thought to primarily represent sensory information. This, in turn, suggests that these computations might be largely invariant to the kind of information they receive. Put another way, selection that is implemented by flexible readout should be similarly strong or efficient regardless of the feature selected for. In Aim 2 we sought to validate this prediction by directly comparing spatial and feature-based attention. I developed two variants of an estimation task for this purpose, one using perceptual averaging and a second working memory. Each task was designed to measure how the strength of sensory selection changes according to the feature being selected. I showed with these tasks that there are only subtle differences between spatial selection and feature-based selection whether by motion direction or by color. Importantly, all of these small differences were well accounted for by errors in bias and not changes in sensitivity. This suggests the fascinating possibility that different selection behaviors are all implemented by a common selection mechanism. 

Bringing the findings in this dissertation together, these results hint at the possibility that selection is in large part implemented during the readout stage from sensory representation to a context-dependent one. What then is the role of small changes to the sensory representation, which clearly occur during attentional behaviors? I would suggest that these changes to sensory representations might play a role in selection, but they would be complimentary to this readout process. Much of the recent work on attention aligns with this idea that attention is a two-step process in which sensory changes work together with the readout process to improve perceptual abilities \citep{Pestilli2011-gi,Ruff2018-bm,Snyder2018-yr,Rabinowitz2015-uz}.

This hypothesis appears to echo the ideas of a late selection account of selective attention \citep{Deutsch1963-ac}, but they differ in crucial ways. In the late selection theory sensory processing runs to completion irregardless of an organism's behavioral goals. This matches with the results in Chapter 3, where I showed that observers retain information about unattended features. This would also be consistent with similar results for visual processing of faces and scenes \citep{Li2002-ji,Reddy2006-ua}. But in each of these cases the remaining perceptual representations are impoverished. Even if processing is going to `completion', the sensory representation has degraded considerably by the time the readout process can be shifted to it. In contrast to a late selection account, what I have shown here is more in line with the idea of a continuous selection process.

One explanation for why sensory selection might occur continuously during sensory processing and readout is that this balances the efficiency of processing against behavioral flexibility. Attention is often suggested to be part of a solution to the high cost of neural activity \citep{Lennie2003-ee}. Shifts in tuning which allocate additional processing to attended features would seem to correspond to such a theory. But to effect such a change requires intervening on the sensory representation directly at the cost of lost selectivity for unattended stimuli \citep{Mack1998-nq}. These feature-specific computations are also potentially complex: is the visual cortex structured in such a way that any arbitrary feature can be enhanced? One way to reconcile the need for efficiency against the complexity of implementation is to assume that attention is not a static computation, but one that is modified with experience. Give sufficient time and consistency in a task, it's possible that the human observers in Chapter 3 might have been able to learn a sensory-change implementation to solve that task. In theory such an implementation should be more efficient compared to always representing the entire stimulus and then selecting out the important information at the last step. One potential way to study this further would be to compare attentional behaviors in animals, especially mice and non-human primates, with humans. We know that these animals learn in vastly different ways \citep{Birman2015-fj}. Finding differences in their sensory selection behaviors may clue us into the different ways in which selection can be implemented in the brain.