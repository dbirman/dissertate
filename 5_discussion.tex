
In this thesis I have shown evidence in favor of the hypothesis that selective visual attention is not solely the result of top-down changes to sensory representation in cortex. In addition, I have shown preliminary results which suggest that different forms of attention should be considered part of a similar computation. Together, these findings reveal that our common presumption that sensory selection is about sensory change and not readout is likely to be wrong and that this may extend to all forms of sensory selection. These findings were possible because of our use of \textit{computational linking models} which make explicit hypotheses about the connections between neural representation and perception \citep{Barlow1972-kz,Brindley1960-gq,Cohen2010-xs,Newsome1989-fr,Pestilli2011-gi}.

In Aim 1 I built the framework necessary to demonstrate these ideas. Prior to this study nobody had established how motion visibility, defined by contrast and motion coherence, is represented in human visual cortex. We sought to characterize and quantify these effects, so that we could build a computational linking model of motion visibility. 

In Aim 2 I measured how the sensory representation of motion visibility changes during directed attention and demonstrated that these changes were insufficient to account for perception. The first step in this study was to validate that a linking model of motion visibility perception could be built, extending an existing linking model of contrast discrimination \citep{Boynton1999-jd}. We then measured how sensory representations changed and, passing these through the linking model, showed that the scale of changes were too small to account for perception. Based on these observations we suggest that a \textit{flexible readout} must change how signals are gated from sensory cortex into decision-related regions.

The findings in Aim 2 suggest that a substantial amount of the process of sensory selection occurs outside of the areas thought to primarily represent sensory information. This suggests that these computations might be largely invariant to the kind of information they receive, i.e. that selection should be similarly strong or efficient regardless of the feature selected for. In Aim 3 we sought to validate this by directly comparing spatial and feature-based attention. We showed in this section that there are only subtle differences between these, perhaps due to spatial bias. 

Finally, in Aim 4, we demonstrate preliminary results showing that our findings about motion visibility may extend to other features, such as color and motion direction. Motion visibility is represented in the brain by the magnitude of signals in sensory regions. This differs from features like color and motion direction which appear to be encoded by the population activity in different regions. These differences could be important, but the results of Aim 3 suggest that they are not -- if selection by all features is equivalent, we might expect that their implementations in cortex will turn out to share more in common than they will differ. In this final study we lay out the psychophysical task necessary to demonstrate this and early BOLD signal results which hint at this possibility. 

Together, these results suggest that adaptable behaviors depend on a set of computations which 